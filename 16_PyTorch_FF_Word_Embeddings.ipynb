{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "# Uso de word embeddings como representación de datos\n",
    "\n",
    "\n",
    "<img src=\"figs/fig-diagrama-clasificador2.png\" width=\"900\">\n",
    "\n",
    "\n",
    "### 1. **Representar los datos en el modelo de word embeddings seleccionado**:  \n",
    "   - #### Generalmente, solo se tokeniza para separar adecuadamente las palabras.\n",
    "   - #### Sin embargo, dependiendo del modelos de word embeddings algunos preprocesamientos puede mejorar la representación.\n",
    "   - #### Por ejemplo: \n",
    "      - ##### tokenizar y separar correctamente las oraciones y palabras\n",
    "      - ##### convertir a minúsculas\n",
    "      - ##### quitar acentos (dependiendo de la fuente de datos con la que se generaros los embeddings)\n",
    "      - ##### quitar números y puntuación \n",
    "\n",
    "\n",
    "### 3. **Convertir los datos a vectores densos: word embeddings**:  \n",
    "   - #### En el caso de textos corto a nivel de oración, un vector denso por oración. \n",
    "\n",
    "### 4. **Separar los datos para entrenamiento, validación y prueba**:  \n",
    "   - #### Crear los dataset  con la función train_test_split \n",
    "   \n",
    "### 5. **Definir la arquitectura de la red**:  \n",
    "   - Definir una red de 2 capas, con funciones PReLU en las capas ocultas y una capa de salida\n",
    "\n",
    "### 6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "### 7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fasttext\n",
    "\n",
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "#cargar los datos\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el modelo de Word Embeddings y crear los vectores densos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de word embeddings\n",
    "ft = fasttext.load_model('/Volumes/data/temp/MX.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear los vectores densos para cada texto. Se espera una oración corta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>klass</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>@USUARIO Lo que tu no respetas es al país HDP</td>\n",
       "      <td>[-0.047211677, -0.01908352, -0.01999174, 0.024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1536</td>\n",
       "      <td>nonaggressive</td>\n",
       "      <td>no sé cómo pedirle a la señora que me ayuda a ...</td>\n",
       "      <td>[-0.033048205, -0.020297991, 0.025773568, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>nonaggressive</td>\n",
       "      <td>¿Cómo hacer entender en el grupo de la familia...</td>\n",
       "      <td>[-0.045047347, 0.0062094647, -0.0025499659, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>923</td>\n",
       "      <td>nonaggressive</td>\n",
       "      <td>Jaja, perdón por esta vida loca. Nos vemos pro...</td>\n",
       "      <td>[-0.015040112, 0.008109028, 0.009956434, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6075</td>\n",
       "      <td>nonaggressive</td>\n",
       "      <td>Pero que putas hacen esos argentinos hablando ...</td>\n",
       "      <td>[-0.029231621, -0.015333876, 0.0017929205, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id          klass                                               text  \\\n",
       "0    24     aggressive      @USUARIO Lo que tu no respetas es al país HDP   \n",
       "1  1536  nonaggressive  no sé cómo pedirle a la señora que me ayuda a ...   \n",
       "2  2401  nonaggressive  ¿Cómo hacer entender en el grupo de la familia...   \n",
       "3   923  nonaggressive  Jaja, perdón por esta vida loca. Nos vemos pro...   \n",
       "4  6075  nonaggressive  Pero que putas hacen esos argentinos hablando ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.047211677, -0.01908352, -0.01999174, 0.024...  \n",
       "1  [-0.033048205, -0.020297991, 0.025773568, -0.0...  \n",
       "2  [-0.045047347, 0.0062094647, -0.0025499659, 0....  \n",
       "3  [-0.015040112, 0.008109028, 0.009956434, 0.006...  \n",
       "4  [-0.029231621, -0.015333876, 0.0017929205, 0.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracción de los textos en arreglos de numpy\n",
    "# Se aplica una función lambda para obtener el vector de cada texto del conjunto de datos\n",
    "# El resultado es una nueva columna \"embedding\"\n",
    "dataset[\"embedding\"] = dataset[\"text\"].map(lambda x: ft.get_sentence_vector(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear la matriz de datos para el entrenamiento, validación y prueba del modelo de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos: (5132, 300)\n",
      "Etiquetas: (5132,)\n"
     ]
    }
   ],
   "source": [
    "# Cada renglón representa un documento codificado en un texto de embeddings\n",
    "X = np.vstack(dataset['embedding'].to_numpy())\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "print(\"Datos:\", X.shape) \n",
    "print(\"Etiquetas:\", Y.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificar las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:\n",
      "['aggressive' 'nonaggressive']\n",
      "Clases codificadas:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar los conjuntos de datos  para el entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Codificar las clases en forma one-hot \n",
    "NUM_CLASSES = 2\n",
    "Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "\n",
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3694, 300), (3694,), (411, 300), (411,), (1027, 300), (1027,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]]),\n",
       " torch.Size([3694, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_one_hot[:5],  Y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class FF(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 128\n",
    "        input_size_h2 = 8 \n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        self.output = nn.Linear(input_size_h2, output_size)\n",
    "        self.act1= nn.PReLU()\n",
    "        self.act2= nn.PReLU()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)        \n",
    "        if self.output.bias is not None:\n",
    "            nn.init.zeros_(self.output.bias)        \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        x = self.fc1(X)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en PyTorch\n",
      "Batch Error : 0.6770150065422058\n",
      "Batch Error : 0.65428626537323\n",
      "Época 1/10, Pérdida: 0.6450552817048698\n",
      "Época 1/10\n",
      "P= 0.3564476885644769\n",
      "R= 0.5\n",
      "F1= 0.4161931818181818\n",
      "Acc= 0.7128953771289538\n",
      "Batch Error : 0.6243168711662292\n",
      "Batch Error : 0.6543532013893127\n",
      "Batch Error : 0.5273615121841431\n",
      "Batch Error : 0.6213215589523315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/PT/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/admin/opt/anaconda3/envs/PT/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 0.6050225496292114\n",
      "Época 2/10, Pérdida: 0.5899707829130107\n",
      "Época 2/10\n",
      "P= 0.3564476885644769\n",
      "R= 0.5\n",
      "F1= 0.4161931818181818\n",
      "Acc= 0.7128953771289538\n",
      "Batch Error : 0.6083561778068542\n",
      "Batch Error : 0.6114820837974548\n",
      "Batch Error : 0.5589754581451416\n",
      "Batch Error : 0.5854372978210449\n",
      "Batch Error : 0.5097629427909851\n",
      "Época 3/10, Pérdida: 0.5753231521310478\n",
      "Época 3/10\n",
      "P= 0.3564476885644769\n",
      "R= 0.5\n",
      "F1= 0.4161931818181818\n",
      "Acc= 0.7128953771289538\n",
      "Batch Error : 0.6084818243980408\n",
      "Batch Error : 0.5501173734664917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/PT/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 0.5683637261390686\n",
      "Época 4/10, Pérdida: 0.5510002128009138\n",
      "Época 4/10\n",
      "P= 0.7384085213032581\n",
      "R= 0.5330161392954242\n",
      "F1= 0.48830591373943977\n",
      "Acc= 0.7274939172749392\n",
      "Batch Error : 0.5141305327415466\n",
      "Época 5/10, Pérdida: 0.4966999415693612\n",
      "Época 5/10\n",
      "P= 0.7203299750439043\n",
      "R= 0.6378926360849193\n",
      "F1= 0.651453790238837\n",
      "Acc= 0.7615571776155717\n",
      "Batch Error : 0.5271028876304626\n",
      "Batch Error : 0.4228875935077667\n",
      "Batch Error : 0.4410017132759094\n",
      "Batch Error : 0.4043583571910858\n",
      "Batch Error : 0.478652685880661\n",
      "Época 6/10, Pérdida: 0.45055794921414605\n",
      "Época 6/10\n",
      "P= 0.7420896930200651\n",
      "R= 0.6633163648984787\n",
      "F1= 0.6803611280178535\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 0.4568142294883728\n",
      "Batch Error : 0.46392756700515747\n",
      "Época 7/10, Pérdida: 0.42760384904927223\n",
      "Época 7/10\n",
      "P= 0.719383346425766\n",
      "R= 0.6615520333198357\n",
      "F1= 0.6761441985685206\n",
      "Acc= 0.7664233576642335\n",
      "Batch Error : 0.44882920384407043\n",
      "Batch Error : 0.38600748777389526\n",
      "Batch Error : 0.38755857944488525\n",
      "Época 8/10, Pérdida: 0.42028726997046634\n",
      "Época 8/10\n",
      "P= 0.7351119894598155\n",
      "R= 0.6548417886272921\n",
      "F1= 0.6708975057925582\n",
      "Acc= 0.7712895377128953\n",
      "Batch Error : 0.4614713490009308\n",
      "Batch Error : 0.38195285201072693\n",
      "Batch Error : 0.4248979687690735\n",
      "Época 9/10, Pérdida: 0.40759104079213637\n",
      "Época 9/10\n",
      "P= 0.7039919045292763\n",
      "R= 0.669086596864696\n",
      "F1= 0.6803817603393425\n",
      "Acc= 0.7591240875912408\n",
      "Batch Error : 0.2925143837928772\n",
      "Batch Error : 0.40085530281066895\n",
      "Época 10/10, Pérdida: 0.404246620063124\n",
      "Época 10/10\n",
      "P= 0.7102496310870634\n",
      "R= 0.6978076010875225\n",
      "F1= 0.7032049134561698\n",
      "Acc= 0.7639902676399026\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Establecer los parámetros de la red\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size =  X_train.shape[1]\n",
    "\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "epochs = 10 # variar el número de épocas, para probar que funciona la programación \n",
    "                 # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.001 # Generalmente se usan learning rate pequeños (0.001), \n",
    "\n",
    "# Se recomiendan tamaños de batch_size potencias de 2: 16, 32, 64, 128, 256\n",
    "# Entre mayor el número más cantidad de memoria se requiere para el procesamiento\n",
    "batch_size = 128 # definir el tamaño del lote de procesamiento \n",
    "\n",
    "\n",
    "# Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "X_train_t = X_train_t.to(torch.float32)\n",
    "Y_train_t = Y_train_one_hot\n",
    "\n",
    "X_val_t = torch.from_numpy(X_val)\n",
    "X_val_t = X_val_t.to(torch.float32)\n",
    "\n",
    "\n",
    "# Crear la red\n",
    "model = FF(input_size, output_size)\n",
    "\n",
    "# Definir la función de pérdida\n",
    "# Entropía Cruzada \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Definir el optimizador\n",
    "#Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Adaptive Moment Estimation\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        # inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tr)  #invoca al método forward de la clase MLP\n",
    "        # Calcular el error MSE\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        #Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        #  \n",
    "        optimizer.step()\n",
    "        if np.random.random() < 0.1:\n",
    "            print(f\"Batch Error : {loss.item()}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "    \n",
    "    # Evalúa el modelo con el conjunto de validación\n",
    "    model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "    with torch.no_grad():  # No  calcular gradientes \n",
    "        y_pred = model(X_val_t)\n",
    "        # Obtiene una única clase, la más probable\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        print(f\"Época {epoch+1}/{epochs}\")\n",
    "        print(\"P=\", precision_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"R=\", recall_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1,  ..., 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Transformar el dataset de test con los mismos preprocesamientos y al  espacio de \n",
    "# representación vectorial que el modelo entrenado, es decir, al espacio de la matriz TFIDF\n",
    "\n",
    "# Convertir los datos de prueba a tensores de PyTorch\n",
    "\n",
    "X_t = torch.from_numpy(X_test)\n",
    "X_t = X_t.to(torch.float32)\n",
    "\n",
    "# Desactivar el comportamiento de modo de  entrenamiento: por ejemplo, capas como Dropout\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred_test= model(X_t)\n",
    "\n",
    "# y_test_pred contiene las predicciones\n",
    "\n",
    "# Obtener la clase real\n",
    "y_pred_test = torch.argmax(y_pred_test, dim=1)\n",
    "\n",
    "print(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175 121]\n",
      " [ 78 653]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6917    0.5912    0.6375       296\n",
      "           1     0.8437    0.8933    0.8678       731\n",
      "\n",
      "    accuracy                         0.8062      1027\n",
      "   macro avg     0.7677    0.7423    0.7526      1027\n",
      "weighted avg     0.7999    0.8062    0.8014      1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred_test))\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonaggressive' 'aggressive' 'nonaggressive']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_datos = [\"Ese tipo se llevó mis cosas\", \"ese hijo de se llevo el dinero\", \"mi app de calendario no sirve\"]\n",
    "# Transformar los datos a vectores densos: word embeddings\n",
    "x_datos = [ft.get_sentence_vector(texto) for texto in x_datos]\n",
    "\n",
    "# Apilar los vectores verticalmente para tener un ejemplo.\n",
    "vectores = np.vstack(x_datos)\n",
    "\n",
    "x_datos_t = torch.from_numpy(vectores)\n",
    "x_datos_t = x_datos_t.to(torch.float32)\n",
    "\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred = model(x_datos_t)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print(le.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modificar el número de neuronas de las capas: capa1 y capa2 y evaluar si el modelo mejora o empeora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Aplicar una función lamba para que preprocese los textos con los siguientes características:\n",
    "#   - Tokenizar los datos para separar términos que estuvieran juntos por puntuación o símbolos extraños (sugerencia: usar word_tokenizer)\n",
    "#   - convertir a minúsculas\n",
    "#   - eliminar puntuación y símbolos duplicados\n",
    "# Comprobar si el rendimiento del modelo mejora o empeora.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
