{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "# Uso de word embeddings como representación de datos\n",
    "\n",
    "\n",
    "<img src=\"figs/fig-diagrama-clasificador2.png\" width=\"900\">\n",
    "\n",
    "\n",
    "### 1. **Representar los datos en el modelo de word embeddings seleccionado**:  \n",
    "   - #### Generalmente, solo se tokeniza para separar adecuadamente las palabras.\n",
    "   - #### Sin embargo, dependiendo del modelos de word embeddings algunos preprocesamientos puede mejorar la representación.\n",
    "   - #### Por ejemplo: \n",
    "      - ##### tokenizar y separar correctamente las oraciones y palabras\n",
    "      - ##### convertir a minúsculas\n",
    "      - ##### quitar acentos (dependiendo de la fuente de datos con la que se generaros los embeddings)\n",
    "      - ##### quitar números y puntuación \n",
    "\n",
    "\n",
    "### 3. **Convertir los datos a vectores densos: word embeddings**:  \n",
    "   - #### Cada palabra representa un vector, por lo que una oración genera una matriz de vectores\n",
    "\n",
    "### 4. **Separar los datos para entrenamiento, validación y prueba**:  \n",
    "   - #### Crear los dataset  con la función train_test_split \n",
    "   \n",
    "### 5. **Definir la arquitectura de la red**:  \n",
    "   - Definir una red de 1 capa convolucional, una capa maxpooling, una capa completamente conectada como capa de salida\n",
    "\n",
    "### 6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "### 7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fasttext\n",
    "\n",
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "#cargar los datos\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = list(dataset['text'])\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = list(dataset['klass'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el modelo de Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de word embeddings\n",
    "ft = fasttext.load_model('/Volumes/data/temp/MX.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificar las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "Y_encoded = list(Y_encoded)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar los conjuntos de datos  para el entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(Y_train),  len(X_val), len(Y_val), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red convolucional para texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura similar a la propuesta por Yoon Kim\n",
    "\n",
    "<img src=\"figs/fig-red_yoon_kim.png\" width=\"900\">\n",
    "\n",
    "\n",
    "\n",
    "Kim, Y. (2014). Convolutional neural networks for sentence classification. En Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1746-1751). Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definir el modelo\n",
    "class TextConv2D(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_word_length, num_filters, kernel_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.in_channel = 1\n",
    "        self.padding = 0\n",
    "        self.dilatation = 1\n",
    "        self.stride = 1\n",
    "        self.max_word_length = max_word_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.windows_size_maxpool = (2, 2)\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channel, out_channels=num_filters, kernel_size=kernel_size, stride= self.stride, padding=self.padding)\n",
    "        self.pool = nn.MaxPool2d(self.windows_size_maxpool )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        Hout_conv = (((self.max_word_length + 2*self.padding - self.dilatation * (kernel_size[0] -1 ) -1 )  // self.stride) + 1) \n",
    "        Wout_cov = (((embedding_dim +  2*self.padding - self.dilatation * (kernel_size[1] -1 ) -1 )  // self.stride) + 1)\n",
    "\n",
    "        Hout_pool = ((Hout_conv - (2)) // 2)+1  # Para MaxPool, stride por default es el tamaño de la ventana\n",
    "        Wout_pool = ((Wout_cov - (2)) // 2)+1  # Para MaxPool,  stride por default es el tamaño de la ventana\n",
    "\n",
    "        self.fc = nn.Linear(num_filters * Hout_pool * Wout_pool, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forma de entrada de datos\n",
    "        # (Batch_size, Alto, Ancho)        \n",
    "        \n",
    "        # Forma esperada de los datos para CONV2D\n",
    "        # (N,C,H,W)\n",
    "        # (Batch_size, Canales, Alto, Ancho)\n",
    "        x = x.unsqueeze(1)  # Agregar la dimensión del canal\n",
    "\n",
    "        # print(\"después de unsqueeze:\",x.shape)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # print(\"después de relu:\",x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(\"después de pool:\",x.shape)\n",
    "\n",
    "        # Aplanar para la capa totalmente conectada\n",
    "        # x.size(0) mantiene la forma del batch, aplana las demás dimensiones\n",
    "        x = x.view(x.size(0), -1)  \n",
    "\n",
    "        # print(\"después de view:\",x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(\"después de fc:\",x.shape)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forma de Conv2D \n",
    "\n",
    "<img src=\"figs/fig-conv2d_shape.png\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forma de Maxpool2D \n",
    "\n",
    "<img src=\"figs/fig-maxpool2d_shape.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset personalizado para Dataloader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, ft_model, max_length=100, embedding_dim=300):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.ft_model = ft_model\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def sentence_to_embedding(self, sentence):\n",
    "        if (len(sentence.lower().split()) > self.max_length):\n",
    "            print(\"warning oracion mayor, se trunca: \", sentence)\n",
    "        words = sentence.lower().split()[:self.max_length]\n",
    "\n",
    "        embedding_matrix = np.zeros((self.max_length, self.embedding_dim))\n",
    "        for i, word in enumerate(words):\n",
    "            embedding_matrix[i] = self.ft_model.get_word_vector(word)\n",
    "        return embedding_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.sentence_to_embedding(self.sentences[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hiperparámetros\n",
    "num_filters = 5\n",
    "kernel_size = (2, 2)\n",
    "num_classes = 2\n",
    "batch_size=128\n",
    "embedding_dim=300\n",
    "max_word_length = 100\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = TextDataset(X_train, Y_train, ft)\n",
    "val_dataset = TextDataset(X_val, Y_val, ft)\n",
    "test_dataset = TextDataset(X_test, Y_test, ft)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model = TextConv2D(embedding_dim, max_word_length,  num_filters, kernel_size, num_classes)\n",
    "\n",
    "# Definir pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0    \n",
    "    for embeddings, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()    \n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    y_predicted = torch.empty(0)     \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in val_loader:\n",
    "            outputs = model(embeddings)\n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            y_predicted = torch.cat((y_predicted, y_pred)) # Concatenar todas las predicciones de cada batch para al final medir el rendimiento\n",
    "\n",
    "        print(f\"Época {epoch+1}/{epochs}\")\n",
    "        print(\"P=\", precision_score(Y_val, y_predicted, average='macro'))\n",
    "        print(\"R=\", recall_score(Y_val, y_predicted, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_predicted, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación en el conjunto de datos de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO: Evaluación en el conjunto Test\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "y_pred = torch.empty(0) \n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        outputs = model(embeddings)\n",
    "        y_pred_test = torch.argmax(outputs, dim=1)\n",
    "        y_pred = torch.cat((y_pred, y_pred_test))\n",
    "\n",
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred))\n",
    "print(classification_report(Y_test, y_pred, digits=4, zero_division='warn'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_datos = [\"Que basura los put compa\\u00f1eros de Juan\", \"Se llevo el dinero\", \"mi app de calendario no sirve\"]\n",
    "# Transformar los datos a vectores densos: word embeddings\n",
    "y_datos = [-1] * len(x_datos)\n",
    "\n",
    "datos_nuevos = TextDataset(x_datos, y_datos, ft)\n",
    "nuevos_loader = DataLoader(datos_nuevos, batch_size=len(x_datos), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "y_pred = torch.empty(0) \n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in nuevos_loader:\n",
    "        outputs = model(embeddings)\n",
    "        y_pred_test = torch.argmax(outputs, dim=1)\n",
    "        y_pred = torch.cat((y_pred, y_pred_test))\n",
    "\n",
    "y_pred=y_pred.int()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(le.inverse_transform(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modificar el tamaño de los kernels de convolución y evaluar el rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Aumentar  capas completamente conectadas (fully connected) y evaluar el rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Aumentar otra capa de convolución y otra capa de maxpooling y evaluar el rendimiento\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
