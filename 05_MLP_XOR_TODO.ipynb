{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE [[0.03125]\n",
      " [0.03125]\n",
      " [0.03125]\n",
      " [0.03125]]\n",
      "error total 0.125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # TODO: Implementar la derivada de la función sigmoide considerar que el valor x es  sigma(x)\n",
    "    return\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "# Inicializamos los pesos y bias aleatoriamente\n",
    "# Se establece la semilla para la generación de números pseudoaleatorios\n",
    "np.random.seed(42)  # Para reproducibilidad de los experimentos\n",
    "\n",
    "# Pesos entre capa de entrada y capa oculta\n",
    "# Bias de la capa oculta\n",
    "# Pesos entre capa oculta y capa de salida\n",
    "# Bias de la capa de salida\n",
    "W1 = np.zeros((2, 2))   \n",
    "B1 = np.zeros(2)   \n",
    "\n",
    "W2 = np.zeros((1, 2))   \n",
    "B2 = np.zeros(1)   \n",
    "\n",
    "# Definimos la tasa de aprendizaje\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Número de iteraciones de entrenamiento\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 1. Propagación hacia adelante (Forward pass)\n",
    "    #----------------------------------------------\n",
    "\n",
    "    #print('X',X)\n",
    "    #print('W1',W1)\n",
    "    #print('B1',B1)\n",
    "    z_c1 = X @ W1.T + B1\n",
    "    #print('zc1',z_c1)\n",
    "    \n",
    "    a_c1 = sigmoid(z_c1)\n",
    "    #print('ac1',a_c1)\n",
    "\n",
    "    z_c2 = a_c1 @ W2.T\n",
    "    #print('zc2',z_c2)\n",
    "\n",
    "    y_pred = sigmoid(z_c2)\n",
    "    #print('yhat',y_pred)\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # 2. Cálculo del error con MSE\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    error = np.divide((y_pred - y) ** 2, 2 * y.shape[0])\n",
    "    print('MSE',error)\n",
    "\n",
    "    # error total del batch de entrenamiento\n",
    "    total_error =  np.sum(error)\n",
    "    print('error total',total_error)\n",
    "    \"\"\"\n",
    "    #----------------------------------------------\n",
    "    # 3. Propagación hacia atrás (Backward pass)\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # Gradiente de la salida\n",
    "    #----------------------------------------------\n",
    "    # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "\n",
    "    dE_dy_pred = ?\n",
    "    \n",
    "    # TODO: Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "\n",
    "    d_y_pred_d_zc2 = ?\n",
    "\n",
    "    # TODO: Calcular delta de la capa de salida\n",
    "    delta_c2 = ?\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # Gradiente en la capa oculta\n",
    "    #----------------------------------------------\n",
    "    # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "    \n",
    "    delta_c1 =  ? \n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 4. Actualización de los pesos y biases\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "    W2 =  W2 -  ? \n",
    "    B2 =  b2 - ? \n",
    "\n",
    "    # TODO: Actualizar los pesos y bias de la capa oculta\n",
    "    W1 = W1 - ?\n",
    "    B1 = b1 - ?\n",
    "\n",
    "    # Imprimir el error cada 1000 épocas\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Época {epoch}, Error: {total_error}\")\n",
    "\n",
    "\n",
    "\n",
    "# Comprobar los resultados del modelo entrenado\n",
    "# Recordar: al final del entrenamiento los parámetros de la red ya se ajustaron, es decir, las matrices Ws y Bias Bs se usan para predecir nuevos datos através de la red.\n",
    "print(\"\\nResultados finales:\")\n",
    "\n",
    "for i in range(len(X)):\n",
    "    # TODO: Realizar la propagación hacia adelante para cada entrada de prueba\n",
    "    x = X[i]\n",
    "    x = x[np.newaxis, :] # agrega una dimensión al vector x, para que se represente como una matriz de 1 elemento\n",
    "    z_hidden = None # ?    # Suma ponderada de la capa oculta\n",
    "    a_hidden = None # ?    # activación de la neurona\n",
    "    z_output = None # ?    # Suma ponderada de la capa de salida \n",
    "    y_pred = None   # ?    # Predicción para el ejemplo i\n",
    "    \n",
    "    # Mostrar las predicciones\n",
    "    print(f\"Entrada: {X[i]}, Salida estimada: {y_pred}, Salida real: {y[i]}\")\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
