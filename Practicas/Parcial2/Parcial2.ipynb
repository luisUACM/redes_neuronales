{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88480672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import fasttext\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from os import putenv\n",
    "from tqdm import trange\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "#Para usar AMD GPU's con ROC\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"10.3.0\")\n",
    "\n",
    "EXPERIMENTAR = True\n",
    "NUM_CLASES = 3\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4844ad",
   "metadata": {},
   "source": [
    "<h1>Red Neuronal con Pytorch</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac371dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hiden_sizes: list[int], output_size: int):\n",
    "        super().__init__()\n",
    "        self.fcl = nn.ModuleList()\n",
    "        self.act = nn.ModuleList()\n",
    "        \n",
    "        # Las capas se forman por pares de numeros, en total 1 par menos que la lista neuronas\n",
    "        neuronas = [input_size] + hiden_sizes + [output_size]\n",
    "        for i in range(len(neuronas) - 1):\n",
    "            self.fcl.append(nn.Linear(neuronas[i], neuronas[i + 1]))\n",
    "            nn.init.xavier_uniform_(self.fcl[i].weight) # type: ignore\n",
    "            nn.init.zeros_(self.fcl[i].bias) # type: ignore\n",
    "        #Al aplicar CrossEntropy se necesita una función de activación menos que de capas\n",
    "        for i in range(len(neuronas) - 2):\n",
    "            self.act.append(nn.ReLU())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        for i in range(len(self.act)):\n",
    "            x = self.fcl[i](x)\n",
    "            x = self.act[i](x)\n",
    "        x = self.fcl[-1](x)     #No activar la última capa\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87da99",
   "metadata": {},
   "source": [
    "<h1>Funciones de utilidad</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "    SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "    NUMBERS= \"0123456789\"\n",
    "    SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def eliminar_stopwords(texto: str):\n",
    "    tokens = [t for t in texto.split() if t not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "def aplicar_stemming(texto: str):\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    tokens = [stemmer.stem(t) for t in texto.split()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocesar(texto: str):\n",
    "    texto = normaliza_texto(texto)\n",
    "    texto = eliminar_stopwords(texto)\n",
    "    texto = aplicar_stemming(texto)\n",
    "    return texto\n",
    "\n",
    "def leer_datos(filename, transform=False):\n",
    "    dataset = pd.read_json(filename, lines=True)\n",
    "    X = dataset['text']\n",
    "    if transform:\n",
    "        X = X.to_numpy()\n",
    "    Y = dataset['klass'].to_numpy()\n",
    "    return X, Y\n",
    "\n",
    "def vectorizar_TF_IDF(X, Y, test_size=0.2, val_size=0.1, ngram_range=(1,2)):   \n",
    "    vec = TfidfVectorizer(analyzer='word', \n",
    "                        preprocessor=preprocesar,\n",
    "                        ngram_range=ngram_range)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size=test_size, stratify=Y, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=val_size, stratify=Y_train, random_state=RANDOM_STATE)\n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    \n",
    "    return X_train_vec, X_test_vec, X_val_vec, Y_train, Y_test, Y_val\n",
    "\n",
    "def vectorizar_embeddings(X, Y, variante: str, test_size=0.2, val_size=0.1):\n",
    "    if variante == 'MX':\n",
    "        ft = fasttext.load_model('./fasttext/MX.bin')\n",
    "    elif variante == 'ES':\n",
    "        ft = fasttext.load_model('./fasttext/ES.bin')\n",
    "    elif variante == 'GEN':\n",
    "        ft = fasttext.load_model('./fasttext/cc.es.300.bin')\n",
    "\n",
    "    X_ = X.map(lambda x : ft.get_sentence_vector(x)) # type: ignore\n",
    "    X_ = np.vstack(X_.to_numpy())    \n",
    "    X_train, X_test, Y_train, Y_test =  train_test_split(X_, Y, test_size=test_size, stratify=Y, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=val_size, stratify=Y_train, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return X_train, X_test, X_val, Y_train, Y_test, Y_val\n",
    "    \n",
    "def torchificar(X: list, Y: list, representacion: str, one_hot_encoding=False):\n",
    "    le = LabelEncoder()\n",
    "    X_torch = []\n",
    "    Y_torch = []\n",
    "    \n",
    "    for x in X:\n",
    "        if representacion == 'TF-IDF':\n",
    "            x_np = x.toarray().astype(np.float32)\n",
    "        elif representacion == 'Embeddings':\n",
    "            x_np = x.astype(np.float32)\n",
    "        x_torch = torch.from_numpy(x_np) # type: ignore\n",
    "        if torch.cuda.is_available():\n",
    "            x_torch = x_torch.cuda()\n",
    "        X_torch.append(x_torch)\n",
    "    \n",
    "    for y in Y:\n",
    "        y_vec = le.fit_transform(y)\n",
    "        y_torch = torch.from_numpy(y_vec)\n",
    "        if torch.cuda.is_available():\n",
    "            y_torch = y_torch.cuda()\n",
    "        Y_torch.append(y_torch)\n",
    "    if one_hot_encoding:\n",
    "        Y_torch[0] = one_hot(Y_torch[0], num_classes=NUM_CLASES).float()\n",
    "    \n",
    "    return X_torch, Y_torch\n",
    "\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def entrenar(Xs, Ys, modelo: FeedForwardNeuralNetwork, optimizador: optim.Optimizer, funcion_perdida, epocas=100, batch_size=32, verbose=False):\n",
    "    historial_perdida = []\n",
    "    if torch.cuda.is_available():\n",
    "        modelo.cuda()\n",
    "    \n",
    "    for _ in trange(epocas, desc=f'Entrenando'):\n",
    "        modelo.train()\n",
    "        lossTotal = 0   \n",
    "        dataloader = create_minibatches(Xs[0], Ys[0], batch_size=batch_size) #Xs[0] = X_train\n",
    "        \n",
    "        for X_tr, Y_tr in dataloader:\n",
    "            optimizador.zero_grad()\n",
    "            Y_pred = modelo(X_tr)\n",
    "            loss = funcion_perdida(Y_pred, Y_tr)\n",
    "            Y_pred = torch.softmax(Y_pred, dim=1)\n",
    "            Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "            lossTotal += loss.item()\n",
    "            loss.backward()\n",
    "            optimizador.step()\n",
    "                    \n",
    "        perdida = lossTotal/len(dataloader)\n",
    "        historial_perdida.append(perdida)\n",
    "        if verbose:\n",
    "            f1, _, _, _ = evaluar(Xs[2], Ys[2], modelo)\n",
    "            print('Pérdida: ', perdida)\n",
    "            print('F1-score: ', f1)\n",
    "    \n",
    "    return historial_perdida\n",
    "\n",
    "def evaluar(X, Y, modelo: FeedForwardNeuralNetwork, verbose=False):\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred = modelo(X)\n",
    "        Y_pred = torch.softmax(Y_pred, dim=1)\n",
    "        Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            Y_ = Y.cpu()\n",
    "            Y_pred = Y_pred.cpu()\n",
    "        else:\n",
    "            Y_ = Y\n",
    "        \n",
    "        f1 = f1_score(Y_, Y_pred, average='macro')\n",
    "        a = accuracy_score(Y_, Y_pred)\n",
    "        p = precision_score(Y_, Y_pred, average='macro')\n",
    "        r = recall_score(Y_, Y_pred, average='macro')\n",
    "        if verbose:\n",
    "            print(\"F1-score: \", f1)\n",
    "            print(\"Accuracy: \", a)\n",
    "            print(\"Precision: \", p)\n",
    "            print(\"Recall: \", r)\n",
    "    return f1, a, p, r\n",
    "\n",
    "def guardar_modelo(modelo: FeedForwardNeuralNetwork, caracteristicas: dict):\n",
    "    torch.save(modelo.state_dict(), f'./modelos/{caracteristicas['ID']}.pth')\n",
    "    with open('DiccionarioModelos.txt', 'a') as file:\n",
    "        file.write(f'{caracteristicas}\\n')\n",
    "\n",
    "def cargar_modelo(caracteristicas: dict, gpu=True):\n",
    "    modelo = FeedForwardNeuralNetwork(caracteristicas['Entradas'], caracteristicas['Arquitectura'], caracteristicas['Salidas'])\n",
    "    modelo.load_state_dict(torch.load(f'./modelos/{caracteristicas['ID']}.pth', weights_only=True))\n",
    "    if torch.cuda.is_available() and gpu:\n",
    "        modelo.cuda()\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff6029",
   "metadata": {},
   "source": [
    "<h1>Experimentación</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './dataset_polaridad_es.json'\n",
    "epocas = 100\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [32, 16]\n",
    "arquitectura_5_capas = [1024, 512, 256, 128, 64]\n",
    "arquitectura_4_capas = [512, 256, 128, 64]\n",
    "arquitectura_3_capas = [256, 128, 64]\n",
    "arquitecturas = [arquitectura_5_capas, arquitectura_4_capas, arquitectura_3_capas]\n",
    "variantes_esp = ['GEN', 'MX', 'ES']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244d32b",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENTAR:\n",
    "    vec = 'TF-IDF'\n",
    "    n_modelo = 1\n",
    "    X, Y = leer_datos(filename, transform=True)\n",
    "    X_train, X_test, X_val, Y_train, Y_test, Y_val = vectorizar_TF_IDF(X, Y)\n",
    "    Xs, Ys = torchificar([X_train, X_test, X_val], [Y_train, Y_test, Y_val], vec, one_hot_encoding=True) # type: ignore\n",
    "\n",
    "    for arq in arquitecturas:\n",
    "        for lr in learning_rates:\n",
    "            for b in batch_sizes:\n",
    "                caracteristicas = {\n",
    "                    'ID':n_modelo,\n",
    "                    'Vectorizacion':vec,\n",
    "                    'Variante':None,\n",
    "                    'Entradas':Xs[0].shape[1],\n",
    "                    'Arquitectura':arq,\n",
    "                    'Salidas':NUM_CLASES,\n",
    "                    'LearningRate':lr,\n",
    "                    'BatchSize':b,\n",
    "                    'CapasOcultas':len(arq)\n",
    "                }\n",
    "                modelo = FeedForwardNeuralNetwork(Xs[0].shape[1], arq, NUM_CLASES)\n",
    "                funcion_perdida = nn.CrossEntropyLoss()\n",
    "                optimizador = optim.Adam(modelo.parameters(), lr=lr)\n",
    "                historial_perdida = entrenar(Xs, Ys, modelo, optimizador, funcion_perdida, epocas=epocas, batch_size=b)\n",
    "                f1, a, p, r = evaluar(Xs[1], Ys[1], modelo, verbose=True)\n",
    "                caracteristicas.update({\n",
    "                    'Epocas':epocas,\n",
    "                    'F1-score':f1,\n",
    "                    'Accuracy':a,\n",
    "                    'Precision':p,\n",
    "                    'Recall':r,\n",
    "                    'Historial':historial_perdida\n",
    "                })\n",
    "                guardar_modelo(modelo, caracteristicas)\n",
    "                n_modelo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b7b48",
   "metadata": {},
   "source": [
    "<h2>Word Embeddings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENTAR:\n",
    "    vec = 'Embeddings'\n",
    "    X, Y = leer_datos(filename)\n",
    "    for v in variantes_esp:\n",
    "        X_train, X_test, X_val, Y_train, Y_test, Y_val = vectorizar_embeddings(X, Y, v)\n",
    "        Xs, Ys = torchificar([X_train, X_test, X_val], [Y_train, Y_test, Y_val], vec, one_hot_encoding=True) # type: ignore\n",
    "        for arq in arquitecturas:\n",
    "            for lr in learning_rates:\n",
    "                for b in batch_sizes:\n",
    "                    caracteristicas = {\n",
    "                        'ID':n_modelo,\n",
    "                        'Vectorizacion':vec,\n",
    "                        'Variante':v,\n",
    "                        'Entradas':Xs[0].shape[1],\n",
    "                        'Arquitectura':arq,\n",
    "                        'Salidas':NUM_CLASES,\n",
    "                        'LearningRate':lr,\n",
    "                        'BatchSize':b,\n",
    "                        'CapasOcultas':len(arq),\n",
    "                    }\n",
    "                    modelo = FeedForwardNeuralNetwork(Xs[0].shape[1], arq, NUM_CLASES)\n",
    "                    funcion_perdida = nn.CrossEntropyLoss()\n",
    "                    optimizador = optim.Adam(modelo.parameters(), lr=lr)\n",
    "                    historial_perdida = entrenar(Xs, Ys, modelo, optimizador, funcion_perdida, epocas=epocas, batch_size=b)\n",
    "                    f1, a, p, r = evaluar(Xs[1], Ys[1], modelo, verbose=True)\n",
    "                    caracteristicas.update({\n",
    "                        'Epocas':epocas,\n",
    "                        'F1-score':f1,\n",
    "                        'Accuracy':a,\n",
    "                        'Precision':p,\n",
    "                        'Recall':r,\n",
    "                        'Historial':historial_perdida\n",
    "                    })\n",
    "                    guardar_modelo(modelo, caracteristicas)\n",
    "                    n_modelo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0577f4",
   "metadata": {},
   "source": [
    "<h2>Graficación y análisis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_diccionarios(filename):\n",
    "    file = open(filename, 'r')\n",
    "    diccionarios = []\n",
    "    for line in file.readlines():   \n",
    "        diccionarios.append(eval(line))\n",
    "    file.close()\n",
    "    return diccionarios\n",
    "\n",
    "def graficar_conjunto(listas_modelos, nombres_labels, titulo, filename, epocas, opacidad=0.2, error_max=2):\n",
    "    \"\"\"\n",
    "    Grafica un conjunto de redes neuronales en la misma grafica\n",
    "    \"\"\"\n",
    "    colores = ['black', 'darkred', 'forestgreen', 'darkviolet', 'darkorange', 'royalblue']\n",
    "    marcadores = ['solid', (0, (3, 1, 1, 1)), 'dotted', (0, (5, 1)), 'dashed', (0, (5, 10)), (5, (10, 3)), (0, (3, 5, 1, 5))]\n",
    "    i = 0\n",
    "    offset = len(colores) - len(listas_modelos)\n",
    "    patches = []\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title(titulo)\n",
    "    n = 0\n",
    "    for lista in listas_modelos:    \n",
    "        color = colores[i + offset]\n",
    "        for diccionario in lista:\n",
    "            if np.array(diccionario['Historial']).max() > error_max:\n",
    "                n += 1\n",
    "            else:\n",
    "                plt.plot(np.arange(epocas), diccionario['Historial'][:epocas], color=color, linestyle=marcadores[i], linewidth=1, alpha=opacidad)\n",
    "        patches.append(mpatches.Patch(color=color, label=nombres_labels[i]))\n",
    "        i += 1\n",
    "    plt.figtext(0.13, 0.03, f'*{n} modelos excluidos para mejorar la visualización')\n",
    "    plt.legend(handles=patches)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Cross Entropy Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def graficar_metricas(datos, titulo, filename: str):\n",
    "    nombres_metricas = ['F1-score', 'Accuracy', 'Precision', 'Recall']\n",
    "    _, ax = plt.subplots(figsize=(16, 9))\n",
    "    width = 0.2\n",
    "    grupos = np.arange(len(datos[0]))\n",
    "    i = 0\n",
    "    for metrica_p in datos:\n",
    "        offset = width * i\n",
    "        rects = ax.bar(grupos + offset, metrica_p, width, label=nombres_metricas[i])\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        i += 1\n",
    "    ax.set_xticks(grupos + 3 * width / 2, ['TF-IDF', 'Embeddings'])\n",
    "    ax.set_ylabel('Puntuación')\n",
    "    ax.set_title(titulo)\n",
    "    ax.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "diccionarios = leer_diccionarios('DiccionarioModelos.txt')\n",
    "modelos_TF_IDF = [d for d in diccionarios if d['Vectorizacion'] == 'TF-IDF']\n",
    "modelos_embeddings = [d for d in diccionarios if d['Vectorizacion'] == 'Embeddings']\n",
    "graficar_conjunto([modelos_TF_IDF, modelos_embeddings], \n",
    "                  ['TF-IDF', 'Word Embeddings'], \n",
    "                  'TF-IDF vs Word Embeddings', \n",
    "                  './comparacion.png', \n",
    "                  epocas, \n",
    "                  opacidad=0.6)\n",
    "\n",
    "modelos_lr_1 = [d for d in diccionarios if d['Vectorizacion'] == 'Embeddings' and d['LearningRate'] == 0.1]\n",
    "modelos_lr_01 = [d for d in diccionarios if d['Vectorizacion'] == 'Embeddings' and d['LearningRate'] == 0.01]\n",
    "modelos_lr_001 = [d for d in diccionarios if d['Vectorizacion'] == 'Embeddings' and d['LearningRate'] == 0.001]\n",
    "graficar_conjunto([modelos_lr_1, modelos_lr_01, modelos_lr_001], \n",
    "                  ['Learning Rate = 0.1', 'Learning Rate = 0.01', 'Learning Rate = 0.001'], \n",
    "                  'Learning Rates en word embeddings', \n",
    "                  './learningrates.png',\n",
    "                  epocas,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "modelos_arq_3 = [d for d in diccionarios if d['CapasOcultas'] == 3]\n",
    "modelos_arq_4 = [d for d in diccionarios if d['CapasOcultas'] == 4]\n",
    "modelos_arq_5 = [d for d in diccionarios if d['CapasOcultas'] == 5]\n",
    "graficar_conjunto([modelos_arq_3, modelos_arq_4, modelos_arq_5], \n",
    "                  ['3 capas ocultas', '4 capas ocultas', '5 capas ocultas'],\n",
    "                  'Arquitecturas word embeddings', \n",
    "                  './arquitecturas.png',\n",
    "                  epocas,\n",
    "                  opacidad=0.9)\n",
    "\n",
    "# Promedios de métricas\n",
    "\n",
    "metricas_TF_IDF = [(d['F1-score'], d['Accuracy'], d['Precision'], d['Recall']) for d in modelos_TF_IDF]\n",
    "metricas_TF_IDF = np.array(list(metricas_TF_IDF))\n",
    "avg_TF_IDF = metricas_TF_IDF.mean(axis=0)\n",
    "\n",
    "metricas_embeddings = [(d['F1-score'], d['Accuracy'], d['Precision'], d['Recall']) for d in modelos_embeddings]\n",
    "metricas_embeddings = np.array(list(metricas_embeddings))\n",
    "avg_embeddings = metricas_embeddings.mean(axis=0)\n",
    "\n",
    "avg = np.vstack([avg_TF_IDF, avg_embeddings])\n",
    "graficar_metricas(avg.T, 'Métricas promedio', './metricas.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
