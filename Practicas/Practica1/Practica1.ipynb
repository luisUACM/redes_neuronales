{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MultiLayerPerceptron Class</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, \n",
    "                 num_entradas, \n",
    "                 num_neuronas_ocultas, \n",
    "                 num_salidas, epochs, \n",
    "                 inicialitazion_function, \n",
    "                 activation_function, \n",
    "                 activation_derivative, \n",
    "                 loss_function, \n",
    "                 normalization_function=None, \n",
    "                 batch_size=128, \n",
    "                 learning_rate=0.2, \n",
    "                 random_state=42):\n",
    "\n",
    "        self.random_state = random_state\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initialize = inicialitazion_function\n",
    "        self.activate = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.loss_function = loss_function\n",
    "        self.normalize = normalization_function\n",
    "        \n",
    "        # Inicializar\n",
    "        self.seed(self.random_state)\n",
    "        self.W1 = self.initialize(num_neuronas_ocultas, num_entradas)\n",
    "        self.B1 = np.zeros((1, num_neuronas_ocultas), dtype=np.float128)\n",
    "        self.W2 = self.initialize(num_salidas, num_neuronas_ocultas)\n",
    "        self.B2 = np.zeros((1, num_salidas), dtype=np.float128)\n",
    "\n",
    "    def seed(self, random_state):\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def create_minibatches(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "        X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "        \n",
    "        # Divide los datos en minibatches\n",
    "        for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                    np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Realiza el Forward pass\n",
    "        \"\"\"\n",
    "        if self.normalize != None:\n",
    "            self.X = self.normalize(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.z_c1 = self.X @ self.W1.T + self.B1\n",
    "        self.B1 = np.sum(self.B1, axis=0, keepdims=True)\n",
    "        self.a_c1 = self.activate(self.z_c1)\n",
    "\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.B2\n",
    "        self.B2 = np.sum(self.B2 , axis=0, keepdims=True)\n",
    "        self.y_pred = self.activate(self.z_c2)\n",
    "        return self.y_pred \n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\" \n",
    "        Realiza la Backpropagation\n",
    "        \"\"\"\n",
    "        self.dE_dy_pred = self.y_pred - y\n",
    "        self.dy_pred_dz_c2 = self.activation_derivative(self.y_pred)\n",
    "        \n",
    "        self.delta_c2 = self.dE_dy_pred * self.dy_pred_dz_c2\n",
    "        self.grad_c2 = self.delta_c2.T @ self.a_c1\n",
    "\n",
    "        self.delta_c1 = self.delta_c2 @ self.W2 * self.activation_derivative(self.a_c1)\n",
    "        self.grad_c1 = self.delta_c1.T @ self.X\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Re-ajusta los parámetros de la red\n",
    "        \"\"\"\n",
    "        self.W2 =  self.W2 -  self.learning_rate * self.grad_c2\n",
    "        self.B2 =  self.B2 - self.learning_rate * np.sum(self.delta_c2, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W1 = self.W1 - self.learning_rate * self.grad_c1\n",
    "        self.B1 = self.B1 - self.learning_rate * np.sum(self.delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        Predice como un clasificador binario y establece el acccuracy\n",
    "        \"\"\"\n",
    "        self.y_pred = self.forward(X)\n",
    "        self.y_pred = np.where(self.y_pred >= 0.5, 1, 0)\n",
    "    \n",
    "        errores = np.count_nonzero(self.y_pred - y)\n",
    "        aciertos = y.shape[0] - errores\n",
    "        self.accuracy = aciertos / y.shape[0]\n",
    "        return self.y_pred \n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Entrena a la red neuronal con las features X y el target Y\n",
    "        1.- Forward Pass\n",
    "        2.- Cálculo del error\n",
    "        3.- Backpropagation\n",
    "        4.- Actualización de parámetros\n",
    "        \"\"\"\n",
    "        self.epochs_error = []\n",
    "        if self.normalize != None:\n",
    "            self.X = self.normalize(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "        for epoch in range(self.epochs):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in self.create_minibatches(X, Y, self.batch_size):\n",
    "                self.y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function(self.y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward(y_batch)\n",
    "                self.update()\n",
    "                num_batch += 1\n",
    "            self.epochs_error.append(epoch_error/num_batch)\n",
    "            # Imprimir el error cada N épocas\n",
    "            if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error: {epoch_error/num_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Estrategias de ejecución</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Funciones de activación\n",
    "# -----------------------\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# -----------------------\n",
    "# Funciones de error\n",
    "# -----------------------\n",
    "def loss_function_MSE(y_pred, y):\n",
    "    error = np.divide((y_pred - y) ** 2, 2 * y.shape[0])\n",
    "    total_error =  np.sum(error)\n",
    "    return total_error\n",
    "\n",
    "# ---------------------------\n",
    "# Funciones de inicialización\n",
    "# ---------------------------\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    array = np.random.normal(0, np.sqrt(2 / input_size), (input_size, output_size))\n",
    "    return np.float128(array)\n",
    "    \n",
    "def normal_distribution_initialization(input_size, output_size):\n",
    "    array = np.random.normal(0, 0.1, (input_size, output_size))\n",
    "    return np.float128(array)\n",
    "\n",
    "# --------------------------\n",
    "# Funciones de normalización\n",
    "# --------------------------\n",
    "def z_score_normalization(x):\n",
    "    return (x - np.mean(x)) / np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Leer datos de entrenamiento y prueba</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Compuerta lógica XOR\n",
    "# --------------------------\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "Y_xor = np.array([[0], \n",
    "                  [1], \n",
    "                  [1], \n",
    "                  [0]])\n",
    "\n",
    "# --------------------------\n",
    "# Dataset Iris\n",
    "# --------------------------\n",
    "data = np.genfromtxt('./datasets/iris_train.csv', delimiter=',', skip_header=1)\n",
    "X_iris_train = data[:, :-1]\n",
    "Y_iris_train = data[:, -1]\n",
    "Y_iris_train = Y_iris_train.reshape(len(Y_iris_train),1)\n",
    "data = np.genfromtxt('./datasets/iris_test.csv', delimiter=',', skip_header=1)\n",
    "X_iris_test = data[:, :-1]\n",
    "Y_iris_test = data[:, -1]\n",
    "Y_iris_test = Y_iris_test.reshape(len(Y_iris_test),1)\n",
    "\n",
    "# --------------------------\n",
    "# Dataset Breast Cancer\n",
    "# --------------------------\n",
    "data = np.genfromtxt('./datasets/breast_cancer_train.csv', delimiter=',', skip_header=1)\n",
    "X_breast_cancer_train = data[:, :-1]\n",
    "Y_breast_cancer_train = data[:, -1]\n",
    "Y_breast_cancer_train = Y_breast_cancer_train.reshape(len(Y_breast_cancer_train),1)\n",
    "data = np.genfromtxt('./datasets/breast_cancer_test.csv', delimiter=',', skip_header=1)\n",
    "X_breast_cancer_test = data[:, :-1]\n",
    "Y_breast_cancer_test = data[:, -1]\n",
    "Y_breast_cancer_test = Y_breast_cancer_test.reshape(len(Y_breast_cancer_test),1)\n",
    "\n",
    "# --------------------------\n",
    "# Dataset Wine\n",
    "# --------------------------\n",
    "data = np.genfromtxt('./datasets/wine_train.csv', delimiter=',', skip_header=1)\n",
    "X_wine_train = data[:, :-1]\n",
    "Y_wine_train = data[:, -1]\n",
    "Y_wine_train = Y_wine_train.reshape(len(Y_wine_train),1)\n",
    "data = np.genfromtxt('./datasets/wine_test.csv', delimiter=',', skip_header=1)\n",
    "X_wine_test = data[:, :-1]\n",
    "Y_wine_test = data[:, -1]\n",
    "Y_wine_test = Y_wine_test.reshape(len(Y_wine_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Entrenamiento de los modelos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Datos de entrenamiento\n",
    "# --------------------------\n",
    "X_train = [X_xor, X_iris_train, X_breast_cancer_train, X_wine_train]\n",
    "Y_train = [Y_xor, Y_iris_train, Y_breast_cancer_train, Y_wine_train]\n",
    "X_test = [X_xor, X_iris_test, X_breast_cancer_test, X_wine_test]\n",
    "Y_test = [Y_xor, Y_iris_test, Y_breast_cancer_test, Y_wine_test]\n",
    "neuronas_ocultas = [2, 4, 8, 16, 32, 128]\n",
    "salidas = 1\n",
    "batch_size = [8, 16, 32, 64]\n",
    "learning_rate = [0.01, 0.1, 0.5]\n",
    "epochs = 2000\n",
    "initialization_functions = [xavier_initialization, normal_distribution_initialization]\n",
    "normalization_functions = [z_score_normalization, None]\n",
    "nombres_datasets = ['XOR', 'Iris', 'Breast Cancer', 'Wine']\n",
    "nombres_inicializacion = ['Xavier', 'Normal distribution']\n",
    "nombres_normalizacion = ['Z-score', 'Ninguna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Realizar 288 Experimentos por cada Dataset\n",
    "# --------------------------\n",
    "lista_clasificadores: list[tuple] = []\n",
    "for i in range(len(X_train)):\n",
    "    for i_f_i in range(len(initialization_functions)):\n",
    "        for i_f_n in range(len(normalization_functions)):\n",
    "            for n in neuronas_ocultas:\n",
    "                for lr in learning_rate:\n",
    "                    for b in batch_size:\n",
    "                        diccionario = dict()\n",
    "                        diccionario = {'Dataset': nombres_datasets[i], \n",
    "                                    'Initialization':nombres_inicializacion[i_f_i],\n",
    "                                    'Normalization':nombres_normalizacion[i_f_n],\n",
    "                                    'Activation':'Sigmoid',\n",
    "                                    'Input-size':len(X_train[i][0]),\n",
    "                                    'Output-size':salidas,\n",
    "                                    'Hidden-layer-size':n,\n",
    "                                    'Learning-rate':lr,\n",
    "                                    'Batch-size':b,\n",
    "                                    'Epochs':epochs,\n",
    "                                    }\n",
    "                        print('Entrenando red neuronal con las siguientes características:')\n",
    "                        print(diccionario)\n",
    "                        clasificador = MultiLayerPerceptron(num_entradas=len(X_train[i][0]), \n",
    "                                                            num_neuronas_ocultas=n, \n",
    "                                                            num_salidas=salidas, \n",
    "                                                            inicialitazion_function=initialization_functions[i_f_i], \n",
    "                                                            normalization_function=normalization_functions[i_f_n],\n",
    "                                                            loss_function=loss_function_MSE,\n",
    "                                                            activation_function=sigmoid,\n",
    "                                                            activation_derivative=sigmoid_derivative,\n",
    "                                                            epochs=epochs, \n",
    "                                                            batch_size=b, \n",
    "                                                            learning_rate=lr)\n",
    "                        clasificador.train(X_train[i], Y_train[i])\n",
    "                        lista_clasificadores.append((diccionario, clasificador))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Almacenamiento de los modelos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_modelos(lista_modelos):\n",
    "    file = open('modelos.pkl', 'wb')    \n",
    "    pickle.dump(lista_modelos, file)                    \n",
    "    file.close()  \n",
    "\n",
    "def cargar_modelos():\n",
    "    file = open('modelos.pkl', 'rb')    \n",
    "    lista_modelos = pickle.load(file)\n",
    "    file.close()\n",
    "    return lista_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_modelos(lista_clasificadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_clasificadores: list[tuple[dict, MultiLayerPerceptron]] = cargar_modelos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prueba de los modelos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_clasificadores = [[], [], [], []]\n",
    "for d, c in lista_clasificadores:\n",
    "    i = nombres_datasets.index(d['Dataset'])\n",
    "    c.predict(X_test[i], Y_test[i])\n",
    "    d.update({'Accuracy':c.accuracy, 'Last-error':c.epochs_error[-1]})\n",
    "    if len(mejores_clasificadores[i]) != 0:\n",
    "        if mejores_clasificadores[i][0][0]['Accuracy'] == d['Accuracy']:\n",
    "            mejores_clasificadores[i].append((d, c))\n",
    "        elif mejores_clasificadores[i][0][0]['Accuracy'] < d['Accuracy']:\n",
    "            mejores_clasificadores[i] = [(d, c)]\n",
    "    else:\n",
    "        mejores_clasificadores[i] = [(d, c)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lista in mejores_clasificadores:\n",
    "    print(f'\\n#######Dataset {lista[0][0]['Dataset']}#######\\n')\n",
    "    print(f'Mejor accuracy: {lista[0][0]['Accuracy']}')\n",
    "    if len(lista) > 1:\n",
    "        print(f'Hay {len(lista)} clasificadores con el mismo accuracy')\n",
    "        for d, c in lista:\n",
    "            print(d)\n",
    "    else:\n",
    "        print(lista[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lista de los mejores modelos</h1>\n",
    "<h2>Dataset XOR</h2>\n",
    "<h3>Mejor accuracy: 1.0<br/>\n",
    "Hay 112 clasificadores con el mismo accuracy</h3>\n",
    "<font size='2'>\n",
    "<ul>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0009244202344850484718')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0009244202344850484718')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0009244202344850484718')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0009244202344850484718')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.006825906507797358372')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.006825906507797358372')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.006825906507797358372')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.006825906507797358372')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0007034682531777309165')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0007034682531777309165')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0007034682531777309165')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0007034682531777309165')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.10404053740422085827')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.10404053740422085827')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.10404053740422085827')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.10404053740422085827')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.004341217752180242664')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.004341217752180242664')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.004341217752180242664')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.004341217752180242664')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0004895566860031729911')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0004895566860031729911')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0004895566860031729911')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0004895566860031729911')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0033317845688289904504')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0033317845688289904504')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0033317845688289904504')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0033317845688289904504')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00034108285839430428633')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00034108285839430428633')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00034108285839430428633')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00034108285839430428633')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.11143629462790004088')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.11143629462790004088')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.11143629462790004088')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.11143629462790004088')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0020832410594029358554')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0020832410594029358554')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0020832410594029358554')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0020832410594029358554')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023765149326017377023')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023765149326017377023')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023765149326017377023')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023765149326017377023')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.124239476944594147924')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.124239476944594147924')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.124239476944594147924')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.124239476944594147924')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0021607904195374907425')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0021607904195374907425')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0021607904195374907425')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0021607904195374907425')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00021421495922037760613')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00021421495922037760613')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00021421495922037760613')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00021421495922037760613')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0013673019683460498371')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0013673019683460498371')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0013673019683460498371')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0013673019683460498371')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.06950847859492007098')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.06950847859492007098')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.06950847859492007098')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.06950847859492007098')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011679517903124673413')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011679517903124673413')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011679517903124673413')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011679517903124673413')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025444494498382597797')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025444494498382597797')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025444494498382597797')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025444494498382597797')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008213094036738231356')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008213094036738231356')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008213094036738231356')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008213094036738231356')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.032181302878570813728')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.032181302878570813728')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.032181302878570813728')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.032181302878570813728')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00082412234043520785235')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00082412234043520785235')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00082412234043520785235')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00082412234043520785235')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.008706357984390136641')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.008706357984390136641')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.008706357984390136641')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.008706357984390136641')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000574694410653426024')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000574694410653426024')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000574694410653426024')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000574694410653426024')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.05204906288875691528')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.05204906288875691528')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.05204906288875691528')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.05204906288875691528')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010002588733707364752')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010002588733707364752')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010002588733707364752')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010002588733707364752')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.07455191003795318361')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.07455191003795318361')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.07455191003795318361')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.07455191003795318361')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008669379356473800426')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008669379356473800426')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008669379356473800426')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008669379356473800426')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.021541853173762580541')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.021541853173762580541')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.021541853173762580541')}</li>\n",
    "<li>{'Dataset': 'XOR', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 2, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.021541853173762580541')}</li>\n",
    "</ul>\n",
    "</font>\n",
    "<h2>Dataset Iris</h2>\n",
    "<h3>Mejor accuracy: 1.0<br/>\n",
    "Hay 264 clasificadores con el mismo accuracy</h3>\n",
    "<font size='2'>\n",
    "<ul>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0015620343649324747202')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0014542251250858597148')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010222698162968535253')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00097380162210501163974')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013308102672031491983')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013117489204502800124')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.4582362434891161765e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.9719020250499072716e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3722269814849591713e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.4422459418975833616e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.3184570196125529519e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.2348742182197843992e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011808076715760034978')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010618145254809556923')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0005355704354314458432')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00047402465689468105767')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00010755856680177347947')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000103285233135539663016')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.0932450407487614685e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.46687316634502521e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.8117283655223987212e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.8567312885815924577e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.1187526348988297847e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.022127667030645293e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00021920722853233850606')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00027561280296697018163')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0002810633849895717512')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0003307771715813671592')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.38039663600416956174e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6390311163785510398e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7421204348746734898e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3518915147750944718e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3240001531229691153e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.387489532554544803e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.5928053249512579752e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.846047399066998554e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00035522663497663262835')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00018007022818475599133')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00017398307803881676548')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00020391282741360646183')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.7640764341391022168e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5617187281903709498e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.488358755454295021e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.8235303716727135464e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.3622194594049412085e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.7245506809351281104e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.6431288861902701631e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7364129338543849465e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00017161083543121939025')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00019476034392118298696')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023381280725576427859')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00023467829516617453566')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.3678233054959507649e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.572675431058466925e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2930545631872820922e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3694986538930876075e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1271934778739112187e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.4324819767096420615e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.461200772235995358e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00019264374175243780288')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013888850081863820526')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.2937134189826701565e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.2514162911679207232e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3772676776067900226e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5799168268341456443e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.4413517925910375276e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.9273968666385278604e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.798547566341991018e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.775920777956204615e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008523900185247683851')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008523818218670548507')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000852441123138408223')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008521615959621687315')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.476282497214433949e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.476082376507228718e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.4770527024237961455e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.507446857800370546e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4488714735354250998e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.45502156602702176046e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.45958227630748502626e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.098516725513166342e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00032352166580878246018')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00032353504109426590172')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00032373607709098778257')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00032334082126514514228')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1777899325076516859e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1796739736140430074e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1890521432235509655e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2502151737012196345e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.521326542633785678e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.580578624131385159e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4042810702732401783e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6075798867463569615e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00019453310118866960973')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00019457010074224185339')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0001941809192951972651')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00019484127012220564514')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5312062946702189768e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5700954601206401829e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4547237190156124437e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6238544394528492793e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.0573318847743442093e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.0357587144889810977e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.563999566105839262e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0001023235256738695353')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00010233168273627793217')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00010292571535377140979')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00010245644131280379747')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.1522794034377808664e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.1429568656805459155e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.0564308092192178072e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5588649222966303774e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.6919500591521714156e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.0298058487794893668e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.9042233364225890451e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.1055004782451301313e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.10378744694065921e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.1172556530497035135e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.127632770733950725e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.4928985946298742434e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.057321138234737713e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('8.043040172019289059e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.9561344040412243252e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5298896310329361516e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5317704467565373988e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.5200805848926771156e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.3212263807989979896e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Xavier', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6153133421870108061e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0015956271533923692635')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0014843304348064667737')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0010523500330941401403')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.000998451437914716011')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00012830362140345525456')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00012547987126847158125')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.165918917860192402e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.611586656633021292e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2918922864308879919e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3143617970402994835e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.2621734770528271305e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.1650502929549403912e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.001009169647356339128')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0009265340332464223681')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00052083040521237263225')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00046472199235289649655')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('9.9250632024231245856e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('9.2957932614872807896e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.8627621376713074664e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.2289527981096669172e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7602476700302859009e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7154141401750651102e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('6.6762832973725442358e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.6513316445512198183e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00024670675052776000088')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00031018298468523710968')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00031042537886732240224')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00036144420450455230034')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.3353377842264161149e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6003638549833652773e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.697134810807290818e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2673466936531567442e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.233992309930230268e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2578045056840116672e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.5498919130909718366e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.6916750239816337629e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0005535213167779407275')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00028676451790134186306')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00028695502807498739605')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00033667518229497739506')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('4.154033998037336209e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6583454859409415952e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7371154379797347147e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1330135090405380866e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.194048705570651537e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.5966558206906267989e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.7517227844760766216e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.4695613251771361216e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00033132914842554771328')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00036864445590723284172')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0004151539651575784686')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00041615616696134396765')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.860951896962693118e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2702178660936721033e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.2326387681592405013e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.3029861920850326347e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.5607559642806336164e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.1130141515558435235e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.11323297680104284e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.6018845359541952626e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0011526310616550279791')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0008253200564551670252')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00035720629827542203702')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00029084057121396908735')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00011983297652315533086')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.75719454871101939e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.4992373040236070721e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.8121123568197524525e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.2204298104335576416e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00069934520806422399765')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0006992947312203682505')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00070015075903069439885')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0006989579206114209822')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.213211991693016157e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.2075313409815106715e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.209400741332321572e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('5.1649485436710751524e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.0477919762212937902e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6642386942378088544e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.6512506815363202805e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0002524849818448590624')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00025251526270161703402')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00025312933829231331437')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00025244763164737713207')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.0083211361482220921e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.0114178335052868057e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.0160934353536915407e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.006608992383547523e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.8696697192190204117e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.7136740194373901301e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('7.1065450991647428693e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00018029273478655291328')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00018027117069797335664')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00017968780260981871969')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00018026858959331158667')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.42104641561382298995e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4213314251239489576e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4322769709622924437e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.4429981481156636412e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.6316849029875145404e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.79983675616478214e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.9262726043890078093e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.7339057960924682863e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00016480103973768502409')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00016478542391146996317')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00016575698837830290432')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00016478834294699583084')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.292261088672864777e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.2920672960928351674e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.3039184203161597719e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.29934189442837737025e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3762317513562918456e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.3773107421238525654e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.7706004050823251926e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('3.7049000412032389145e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00014541125272104426325')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00014537028571719400525')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00014544197777380893515')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00014538329879176232474')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.1420094213393455561e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.14118274078304182e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.1417136533580370108e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.13748107462965328525e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.0461875656674395437e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.0210398981354945833e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.4253850938107446934e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013725215574407708119')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013724978611145602372')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.0001364586387316006793')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.00013710615846221348103')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.0630274512987288666e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.06382816889481699755e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.0530458560334836288e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.1, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.066484430458181888e-05')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('1.8373906526049394182e-06')}</li>\n",
    "<li>{'Dataset': 'Iris', 'Initialization': 'Normal distribution', 'Normalization': 'Ninguna', 'Activation': 'Sigmoid', 'Input-size': 4, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.5, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('2.1798150021647170807e-06')}</li>\n",
    "</ul>\n",
    "</font>\n",
    "<h2>Dataset Breast Cancer</h2>\n",
    "<h3>Mejor accuracy: 0.95<br/>\n",
    "Hay 8 clasificadores con el mismo accuracy</h3>\n",
    "<font size='2'>\n",
    "<ul>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 8, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.025851965693099546158')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.023814735872297627602')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.024174934620899202784')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 4, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.025434070957688489179')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.029426853758799152185')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 8, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.028253106502060502968')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 32, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.027447667629961841704')}</li>\n",
    "<li>{'Dataset': 'Breast Cancer', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 30, 'Output-size': 1, 'Hidden-layer-size': 128, 'Learning-rate': 0.01, 'Batch-size': 16, 'Epochs': 2000, 'Accuracy': np.float64(0.956140350877193), 'Last-error': np.longdouble('0.025773039791797582371')}</li>\n",
    "</ul>\n",
    "</font>\n",
    "<h2>Dataset Wine</h2>\n",
    "<h3>Mejor accuracy: 1.0<br/>\n",
    "Hay 3 clasificadores con el mismo accuracy</h3>\n",
    "<font size='2'>\n",
    "<li>{'Dataset': 'Wine', 'Initialization': 'Xavier', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 13, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025808646981718063858')}</li>\n",
    "<li>{'Dataset': 'Wine', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 13, 'Output-size': 1, 'Hidden-layer-size': 2, 'Learning-rate': 0.5, 'Batch-size': 32, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.025658880823260419365')}</li>\n",
    "<li>{'Dataset': 'Wine', 'Initialization': 'Normal distribution', 'Normalization': 'Z-score', 'Activation': 'Sigmoid', 'Input-size': 13, 'Output-size': 1, 'Hidden-layer-size': 16, 'Learning-rate': 0.5, 'Batch-size': 64, 'Epochs': 2000, 'Accuracy': np.float64(1.0), 'Last-error': np.longdouble('0.03513544955063513984')}</li>\n",
    "</ul>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Graficación de los modelos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar(diccionario, mlp):\n",
    "    \"\"\"\n",
    "    Grafica y guarda la imagen del error conforme a las épocas durante \n",
    "    el entrenamiento de la red neuronal\n",
    "    \"\"\"\n",
    "    filename = f'{diccionario['Dataset']}_'\n",
    "    filename += f'{diccionario['Initialization']}_'\n",
    "    filename += f'{diccionario['Normalization']}_'\n",
    "    filename += f'{diccionario['Hidden-layer-size']}_'\n",
    "    filename += f'{int(diccionario['Learning-rate']*100)}pc_'\n",
    "    filename += f'{diccionario['Batch-size']}.png'\n",
    "    caracteristicas = f'Dataset: {diccionario['Dataset']}, '\n",
    "    caracteristicas += f'Función de Inicialización: {diccionario['Initialization']}, '\n",
    "    caracteristicas += f'Función de Normalización: {diccionario['Normalization']}, '\n",
    "    caracteristicas += f'Neuronas ocultas: {diccionario['Hidden-layer-size']}, '\n",
    "    caracteristicas += f'Learning Rate: {diccionario['Learning-rate']}, '\n",
    "    caracteristicas += f'Batch Size: {diccionario['Batch-size']}'\n",
    "    \n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title(f'Red Neuronal: {diccionario['Dataset']}')\n",
    "    plt.figtext(0.13, 0.03, caracteristicas)\n",
    "    plt.plot(np.arange(mlp.epochs), mlp.epochs_error, color='aqua', linestyle='-', linewidth=1, label='MSE')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./graficas/{filename}')\n",
    "    plt.close()\n",
    "    \n",
    "def graficar_conjunto(listas_modelos, nombres_labels, titulo, caracteristicas, filename, epocas, opacidad=0.2):\n",
    "    \"\"\"\n",
    "    Grafica un conjunto de redes neuronales en la misma grafica\n",
    "    \"\"\"\n",
    "    colores = ['black', 'darkred', 'darkviolet', 'forestgreen', 'darkorange', 'royalblue']\n",
    "    marcadores = ['solid', (0, (3, 1, 1, 1)), 'dotted', (0, (5, 1)), 'dashed', (0, (5, 10)), (5, (10, 3)), (0, (3, 5, 1, 5))]\n",
    "    i = 0\n",
    "    offset = len(colores) - len(listas_modelos)\n",
    "    patches = []\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title(titulo)\n",
    "    plt.figtext(0.13, 0.03, caracteristicas)\n",
    "    for l in listas_modelos:    \n",
    "        color = colores[i + offset]\n",
    "        for t in l:\n",
    "            plt.plot(np.arange(epocas), t[1].epochs_error[:epocas], color=color, linestyle=marcadores[i], linewidth=1, alpha=opacidad)\n",
    "        patches.append(mpatches.Patch(color=color, label=nombres_labels[i]))\n",
    "        i += 1\n",
    "    plt.legend(handles=patches)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Crear y guardar graficas</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Graficar individualmente\n",
    "# --------------------------\n",
    "for d, c in lista_clasificadores:\n",
    "    graficar(d, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Graficar en conjunto\n",
    "# --------------------------\n",
    "\n",
    "# Filtro por cantidad de neuronas de la capa ocultas\n",
    "Iris_neuronas = []\n",
    "labels_Iris_neuronas = []\n",
    "for n in neuronas_ocultas:\n",
    "    Iris_neuronas.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Iris' \n",
    "                          and t[0]['Initialization'] == 'Normal distribution'\n",
    "                          and t[0]['Normalization'] == 'Z-score' \n",
    "                          and t[0]['Hidden-layer-size'] == n])\n",
    "    labels_Iris_neuronas.append(f'{n} neuronas ocultas')\n",
    "graficar_conjunto(listas_modelos=Iris_neuronas,\n",
    "                  nombres_labels=labels_Iris_neuronas,\n",
    "                  titulo='Dataset Iris', \n",
    "                  caracteristicas='Dataset: Iris, Inicialización: Distribución normal, Normalización: Z-score', \n",
    "                  filename='Iris_neuronas.png', \n",
    "                  epocas=300,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "# Filtro por learning rate\n",
    "Iris_learning_rate = []\n",
    "labels_Iris_learning_rate = []\n",
    "for lr in learning_rate:\n",
    "    Iris_learning_rate.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Iris' \n",
    "                               and t[0]['Initialization'] == 'Normal distribution'\n",
    "                               and t[0]['Learning-rate'] == lr])\n",
    "    labels_Iris_learning_rate.append(f'Learning rate: {lr}')\n",
    "graficar_conjunto(listas_modelos=Iris_learning_rate,\n",
    "                  nombres_labels=labels_Iris_learning_rate,\n",
    "                  titulo='Dataset Iris', \n",
    "                  caracteristicas='Dataset: Iris, Inicialización: Distribución normal',\n",
    "                  filename='Iris_learning_rate.png', \n",
    "                  epocas=300,\n",
    "                  opacidad=0.4)\n",
    "\n",
    "# Filtro por tamaño de batch del dataset Cáncer\n",
    "cancer_batch = []\n",
    "labels_cancer_batch = []\n",
    "for b in batch_size:\n",
    "    cancer_batch.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Breast Cancer' \n",
    "                      and t[0]['Initialization'] == 'Normal distribution'\n",
    "                      and t[0]['Normalization'] == 'Z-score' \n",
    "                      and t[0]['Learning-rate'] == 0.1\n",
    "                      and t[0]['Batch-size'] == b])\n",
    "    labels_cancer_batch.append(f'Tamaño de batch: {b}')\n",
    "graficar_conjunto(listas_modelos=cancer_batch,\n",
    "                  nombres_labels=labels_cancer_batch,\n",
    "                  titulo='Dataset Cáncer de mama', \n",
    "                  caracteristicas='Dataset: Cáncer de mama, Inicialización: Distribución normal, Normalización: Z-score, Learning rate 0.1', \n",
    "                  filename='Cancer_batch.png',  \n",
    "                  epocas=100,\n",
    "                  opacidad=0.85)\n",
    "\n",
    "# Filtro por forma de inicialización\n",
    "vino_init = []\n",
    "labels_vino_init = []\n",
    "for init in nombres_inicializacion:\n",
    "    vino_init.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Wine'\n",
    "                      and t[0]['Initialization'] == init\n",
    "                      and t[0]['Normalization'] == 'Z-score' \n",
    "                      and t[0]['Learning-rate'] == 0.01])\n",
    "    labels_vino_init.append(f'Inicialización: {init}')\n",
    "graficar_conjunto(listas_modelos=vino_init,\n",
    "                  nombres_labels=labels_vino_init,\n",
    "                  titulo='Dataset Vino', \n",
    "                  caracteristicas='Dataset: Vino, Normalización: Z-score, Learning rate 0.01', \n",
    "                  filename='Vino_init.png',  \n",
    "                  epocas=200,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "# Filtro por forma de normalización para el dataset de Cáncer de mama\n",
    "cancer_normalizacion = []\n",
    "labels_cancer_normalizacion = []\n",
    "for norm in nombres_normalizacion:\n",
    "    cancer_normalizacion.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Breast Cancer'\n",
    "                                 and t[0]['Normalization'] == norm])\n",
    "    labels_cancer_normalizacion.append(f'Normalización: {norm}')\n",
    "graficar_conjunto(listas_modelos=cancer_normalizacion,\n",
    "                  nombres_labels=labels_cancer_normalizacion,\n",
    "                  titulo='Dataset Cáncer de mama', \n",
    "                  caracteristicas='Dataset: Cáncer de mama', \n",
    "                  filename='Cancer_norm.png',  \n",
    "                  epocas=200)\n",
    "\n",
    "# Filtro por forma de normalización para el dataset de Vino\n",
    "wine_normalizacion = []\n",
    "labels_wine_normalizacion = []\n",
    "for norm in nombres_normalizacion:\n",
    "    wine_normalizacion.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Wine'\n",
    "                                 and t[0]['Normalization'] == norm])\n",
    "    labels_wine_normalizacion.append(f'Normalización: {norm}')\n",
    "graficar_conjunto(listas_modelos=wine_normalizacion,\n",
    "                  nombres_labels=labels_wine_normalizacion,\n",
    "                  titulo='Dataset Vino', \n",
    "                  caracteristicas='Dataset: Vino', \n",
    "                  filename='Vino_norm.png',  \n",
    "                  epocas=200)\n",
    "\n",
    "# Filtro por forma de normalización para el dataset de Iris\n",
    "iris_normalizacion = []\n",
    "labels_iris_normalizacion = []\n",
    "for norm in nombres_normalizacion:\n",
    "    iris_normalizacion.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'Iris'\n",
    "                                 and t[0]['Normalization'] == norm])\n",
    "    labels_iris_normalizacion.append(f'Normalización: {norm}')\n",
    "graficar_conjunto(listas_modelos=iris_normalizacion,\n",
    "                  nombres_labels=labels_iris_normalizacion,\n",
    "                  titulo='Dataset Iris', \n",
    "                  caracteristicas='Dataset: Iris', \n",
    "                  filename='Iris_norm.png',  \n",
    "                  epocas=200)\n",
    "\n",
    "# Filtro por forma de normalización para el dataset de XOR\n",
    "xor_normalizacion = []\n",
    "labels_xor_normalizacion = []\n",
    "for norm in nombres_normalizacion:\n",
    "    xor_normalizacion.append([t for t in lista_clasificadores if t[0]['Dataset'] == 'XOR'\n",
    "                                 and t[0]['Normalization'] == norm])\n",
    "    labels_xor_normalizacion.append(f'Normalización: {norm}')\n",
    "graficar_conjunto(listas_modelos=xor_normalizacion,\n",
    "                  nombres_labels=labels_xor_normalizacion,\n",
    "                  titulo='Dataset XOR', \n",
    "                  caracteristicas='Dataset: XOR', \n",
    "                  filename='XOR_norm.png',\n",
    "                  epocas=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
