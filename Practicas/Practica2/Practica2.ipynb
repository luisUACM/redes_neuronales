{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/luis-\n",
      "[nltk_data]     beto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tqdm import trange\n",
    "from itertools import product\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS_ES = stopwords.words(\"spanish\")\n",
    "STOPWORDS_EN = stopwords.words('english')\n",
    "\n",
    "EXPERIMENTAR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MultiLayerPerceptron Class</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, \n",
    "                 num_entradas, \n",
    "                 num_neuronas_ocultas, \n",
    "                 num_salidas, epochs, \n",
    "                 inicialitazion_function, \n",
    "                 activation_function, \n",
    "                 activation_derivative, \n",
    "                 loss_function, \n",
    "                 normalization_function=None, \n",
    "                 batch_size=128, \n",
    "                 learning_rate=0.2, \n",
    "                 stop_error=0.02,\n",
    "                 rango_epocas=5,\n",
    "                 umbral_mejora=0.003):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initialize = inicialitazion_function\n",
    "        self.activate = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.loss_function = loss_function\n",
    "        self.normalize = normalization_function\n",
    "        self.stop_error = stop_error\n",
    "        self.rango_epocas = rango_epocas\n",
    "        self.umbral_mejora = umbral_mejora\n",
    "        \n",
    "        # Inicializar\n",
    "        self.W1 = self.initialize(num_neuronas_ocultas, num_entradas)\n",
    "        self.B1 = np.zeros((1, num_neuronas_ocultas), dtype=np.float64)\n",
    "        self.W2 = self.initialize(num_salidas, num_neuronas_ocultas)\n",
    "        self.B2 = np.zeros((1, num_salidas), dtype=np.float64)\n",
    "\n",
    "    def create_minibatches(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "        X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "        \n",
    "        # Divide los datos en minibatches\n",
    "        for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                    np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Realiza el Forward pass\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.z_c1 = self.X @ self.W1.T + self.B1\n",
    "        self.B1 = np.sum(self.B1, axis=0, keepdims=True)\n",
    "        self.a_c1 = self.activate(self.z_c1)\n",
    "\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.B2\n",
    "        self.B2 = np.sum(self.B2 , axis=0, keepdims=True)\n",
    "        self.y_pred = self.activate(self.z_c2)\n",
    "        return self.y_pred \n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\" \n",
    "        Realiza la Backpropagation\n",
    "        \"\"\"\n",
    "        self.dE_dy_pred = self.y_pred - y\n",
    "        self.dy_pred_dz_c2 = self.activation_derivative(self.y_pred)\n",
    "        \n",
    "        self.delta_c2 = self.dE_dy_pred * self.dy_pred_dz_c2\n",
    "        self.grad_c2 = self.delta_c2.T @ self.a_c1\n",
    "\n",
    "        self.delta_c1 = self.delta_c2 @ self.W2 * self.activation_derivative(self.a_c1)\n",
    "        self.grad_c1 = self.delta_c1.T @ self.X\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Re-ajusta los parámetros de la red\n",
    "        \"\"\"\n",
    "        self.W2 =  self.W2 -  self.learning_rate * self.grad_c2\n",
    "        self.B2 =  self.B2 - self.learning_rate * np.sum(self.delta_c2, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W1 = self.W1 - self.learning_rate * self.grad_c1\n",
    "        self.B1 = self.B1 - self.learning_rate * np.sum(self.delta_c1, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        Predice como un clasificador binario\n",
    "        \"\"\"\n",
    "        self.y_pred = self.forward(X)\n",
    "        self.y_pred = np.where(self.y_pred >= 0.5, 1, 0)\n",
    "        return self.y_pred \n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Entrena a la red neuronal con las features X y el target Y\n",
    "        1.- Forward Pass\n",
    "        2.- Cálculo del error\n",
    "        3.- Backpropagation\n",
    "        4.- Actualización de parámetros\n",
    "        \"\"\"\n",
    "        self.epochs_error = []\n",
    "        self.X = X\n",
    "        for epoch in trange(self.epochs, desc=\"Entrenando\"):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in self.create_minibatches(X, Y, self.batch_size):\n",
    "                self.y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function(self.y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward(y_batch)\n",
    "                self.update()\n",
    "                num_batch += 1\n",
    "            self.epochs_error.append(epoch_error/num_batch)\n",
    "            \n",
    "            #Criterio de paro\n",
    "            if epoch_error/num_batch < self.stop_error:\n",
    "                print(f\"Paro por error mínimo en época {epoch}\")\n",
    "                self.epochs = epoch + 1\n",
    "                break\n",
    "            elif epoch % self.rango_epocas == 0 and len(self.epochs_error) > 2 * self.rango_epocas:\n",
    "                rango_anterior = np.mean(self.epochs_error[-2 * self.rango_epocas : -self.rango_epocas])\n",
    "                rango_actual = np.mean(self.epochs_error[-self.rango_epocas:])\n",
    "                mejora = rango_anterior - rango_actual\n",
    "                if mejora < self.umbral_mejora:\n",
    "                    print(f\"Paro por falta de mejora en época {epoch}\")\n",
    "                    self.epochs = epoch + 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Estrategias de ejecución</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Funciones de activación\n",
    "# -----------------------\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# -----------------------\n",
    "# Funciones de error\n",
    "# -----------------------\n",
    "def loss_function_MSE(y_pred, y):\n",
    "    error = np.divide((y_pred - y) ** 2, 2 * y.shape[0])\n",
    "    total_error =  np.sum(error)\n",
    "    return total_error\n",
    "\n",
    "# ---------------------------\n",
    "# Funciones de inicialización\n",
    "# ---------------------------\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    x = np.sqrt(6 / (input_size + output_size))\n",
    "    array = np.random.uniform(-x, x, (input_size, output_size))\n",
    "    return np.float64(array)\n",
    "    \n",
    "def normal_distribution_initialization(input_size, output_size):\n",
    "    array = np.random.normal(0, 0.1, (input_size, output_size))\n",
    "    return np.float64(array)\n",
    "\n",
    "# -----------------------------\n",
    "# Funciones de preprocesamiento\n",
    "# -----------------------------\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "    SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "    NUMBERS= \"0123456789\"\n",
    "    SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "    SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def eliminar_stopwords(texto: str, idioma):\n",
    "    if idioma == 'ES':\n",
    "        tokens = [t for t in texto.split() if t not in STOPWORDS_ES]\n",
    "        return ' '.join(tokens)\n",
    "    elif idioma == 'EN':\n",
    "        tokens = [t for t in texto.split() if t not in STOPWORDS_EN]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "def aplicar_stemming(texto: str, idioma):\n",
    "    if idioma == 'ES':\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "    elif idioma == 'EN':\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(t) for t in texto.split()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Esta función se usa para pasar al vectorizador,\n",
    "# Ya que no se puede pasar mas de 1 parametro \n",
    "# Con una función interna se puede acceder a mas de 1 parámetro\n",
    "def preprocesar(preprocesamientos, idioma):\n",
    "    def aplicar_preprocesamiento(texto):\n",
    "        if preprocesamientos[0] != None:\n",
    "            texto = preprocesamientos[0](texto)\n",
    "        if preprocesamientos[1] != None:\n",
    "            texto = preprocesamientos[1](texto, idioma)\n",
    "        if preprocesamientos[2] != None:\n",
    "            texto = preprocesamientos[2](texto, idioma)\n",
    "        return texto\n",
    "    return aplicar_preprocesamiento\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experimentación</h1>\n",
    "<h2>Funciones para el ciclo de experimentación</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(textos, vectorizador, ngram_range, preprocesamientos, idioma):\n",
    "    if vectorizador == 'TF':\n",
    "        vec = CountVectorizer(analyzer='word', \n",
    "                              preprocessor=preprocesar(preprocesamientos, idioma),\n",
    "                              ngram_range=ngram_range)\n",
    "    elif vectorizador == 'TF-IDF':\n",
    "        vec = TfidfVectorizer(analyzer='word', \n",
    "                              preprocessor=preprocesar(preprocesamientos, idioma),\n",
    "                              ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(textos)\n",
    "    return vec, X\n",
    "\n",
    "def guardar_modelos(tupla, filename):\n",
    "    file = open(filename, 'ab')\n",
    "    pickle.dump(tupla, file)                    \n",
    "    file.close()\n",
    "\n",
    "def graficar(diccionario, mlp):\n",
    "    \"\"\"\n",
    "    Grafica y guarda la imagen del error conforme a las épocas durante \n",
    "    el entrenamiento de la red neuronal\n",
    "    \"\"\"\n",
    "    filename = f'{diccionario['Dataset']}_'\n",
    "    filename += f'{diccionario['Pesado']}_'\n",
    "    filename += f'{diccionario['N-gramas']}_'\n",
    "    filename += f'{diccionario['Preprocesamiento']}_'\n",
    "    filename += f'{diccionario['Initialization']}_'\n",
    "    filename += f'{diccionario['Hidden-layer-size']}_'\n",
    "    filename += f'lr{int(diccionario['Learning-rate']*100)}_'\n",
    "    filename += f'{diccionario['Batch-size']}.png'\n",
    "    caracteristicas = f'Dataset: {diccionario['Dataset']}, '\n",
    "    caracteristicas += f'Método de pesado: {diccionario['Pesado']}, '\n",
    "    caracteristicas += f'Representación de términos: {diccionario['N-gramas']}, '\n",
    "    caracteristicas += f'Preprocesamiento de datos: {diccionario['Preprocesamiento']}\\n'\n",
    "    caracteristicas += f'Función de Inicialización: {diccionario['Initialization']}, '\n",
    "    caracteristicas += f'Neuronas ocultas: {diccionario['Hidden-layer-size']}, '\n",
    "    caracteristicas += f'Learning Rate: {diccionario['Learning-rate']}, '\n",
    "    caracteristicas += f'Batch Size: {diccionario['Batch-size']}'\n",
    "    \n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title(f'Reconocimiento de discurso de odio {diccionario['Dataset']}')\n",
    "    plt.figtext(0.13, 0.02, caracteristicas)\n",
    "    plt.plot(np.arange(mlp.epochs), mlp.epochs_error, color='aqua', linestyle='-', linewidth=1, label='MSE')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./graficas/{filename}')\n",
    "    plt.close()\n",
    "\n",
    "def test_modelo(modelo: MultiLayerPerceptron, vectorizador: CountVectorizer | TfidfVectorizer, X, Y_test):\n",
    "    X_vec = vectorizador.transform(X)\n",
    "    y_pred = modelo.predict(X_vec,Y_test)\n",
    "    \n",
    "    a = accuracy_score(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred, average='macro')\n",
    "    pr = precision_score(Y_test, y_pred, average='macro')\n",
    "    rc = recall_score(Y_test, y_pred, average='macro')\n",
    "    print(f'F1-score: {f1}')\n",
    "    print(f'Precision: {pr}')\n",
    "    print(f'Recall: {rc}')\n",
    "    print(f'Accuracy: {a}')\n",
    "    return a, f1, pr, rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Datos de entrenamiento y prueba</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENTAR:\n",
    "    # --------------------------\n",
    "    # Documentos en inglés\n",
    "    # --------------------------\n",
    "    docs = pd.read_json('./datasets/hateval_en_train.json', lines=True)\n",
    "    textos_en_train = docs['text']\n",
    "    Y_en_train = docs['klass']\n",
    "    Y_en_train = Y_en_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    docs = pd.read_json('./datasets/hateval_en_test.json', lines=True)\n",
    "    textos_en_test = docs['text']\n",
    "    Y_en_test = docs['klass']\n",
    "    Y_en_test = Y_en_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    docs = pd.read_json('./datasets/hateval_en_all.json', lines=True)\n",
    "    textos_en_all = docs['text']\n",
    "    Y_en_all = docs['klass']\n",
    "    Y_en_all = Y_en_all.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # --------------------------\n",
    "    # Documentos en español\n",
    "    # --------------------------\n",
    "    docs = pd.read_json('./datasets/hateval_es_train.json', lines=True)\n",
    "    textos_es_train = docs['text']\n",
    "    Y_es_train = docs['klass']\n",
    "    Y_es_train = Y_es_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    docs = pd.read_json('./datasets/hateval_es_test.json', lines=True)\n",
    "    textos_es_test = docs['text']\n",
    "    Y_es_test = docs['klass']\n",
    "    Y_es_test = Y_es_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    docs = pd.read_json('./datasets/hateval_es_all.json', lines=True)\n",
    "    textos_es_all = docs['text']\n",
    "    Y_es_all = docs['klass']\n",
    "    Y_es_all = Y_es_all.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # --------------------------\n",
    "    # Datos de entrenamiento\n",
    "    # --------------------------\n",
    "    textos_train = [textos_en_train, textos_es_train]\n",
    "    Ys_train = [Y_en_train, Y_es_train]\n",
    "    textos_test = [textos_en_test, textos_es_test]\n",
    "    Ys_test = [Y_en_test, Y_es_test]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Variables de los experimentos\n",
    "    # -----------------------------\n",
    "    pesados = ['TF', 'TF-IDF']\n",
    "    n_gramas = [(1,1), (2, 2), (1,2)]\n",
    "    preprocesamientos = [(normaliza_texto, None, None), (normaliza_texto, eliminar_stopwords, None), (normaliza_texto, eliminar_stopwords, aplicar_stemming)]\n",
    "    initialization_functions = [xavier_initialization, normal_distribution_initialization]\n",
    "    neuronas_ocultas = [64, 128, 256, 512, 1024]\n",
    "    learning_rate = [0.01, 0.1, 0.5]\n",
    "    batch_size = [16, 32, 64]\n",
    "    epochs = 100\n",
    "    salidas = 1\n",
    "\n",
    "    nombres_datasets = ['EN', 'ES']\n",
    "    nombres_n_gramas = ['Unigramas', 'Bigramas', 'Unigramas+Bigramas']\n",
    "    nombres_preprocesamientos = ['Normalizacion', 'Normalizacion+Stopwords', 'Normalizacion+Stopwords+Stemming']\n",
    "    nombres_inicializacion = ['Xavier', 'Normal distribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selección aleatoria de experimentos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENTAR:\n",
    "    total_experimentos = []\n",
    "    for x in product(pesados, \n",
    "                    zip(n_gramas, nombres_n_gramas), \n",
    "                    zip(preprocesamientos, nombres_preprocesamientos), \n",
    "                    zip(initialization_functions, nombres_inicializacion),\n",
    "                    neuronas_ocultas,\n",
    "                    learning_rate,\n",
    "                    batch_size):\n",
    "        total_experimentos.append(x)\n",
    "    random.seed(RANDOM_STATE)\n",
    "    experimentos = random.sample(total_experimentos, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ciclo principal</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Realizar experimentos por cada Dataset\n",
    "# --------------------------------------\n",
    "if EXPERIMENTAR:\n",
    "    n_modelos = 0\n",
    "\n",
    "    for i in range(len(textos_train)):\n",
    "        nombre_dataset = nombres_datasets[i]\n",
    "        textos = textos_train[i]\n",
    "        Y = Ys_train[i]\n",
    "        X_test = textos_test[i]\n",
    "        Y_test = Ys_test[i]\n",
    "        for i_p in range(len(pesados)):\n",
    "            pesado = pesados[i_p]\n",
    "            for i_ngram in range(len(n_gramas)):\n",
    "                nombre_grama = nombres_n_gramas[i_ngram]\n",
    "                n_grama = n_gramas[i_ngram]\n",
    "                for i_proc in range(len(preprocesamientos)):            \n",
    "                    nombre_preprocesamiento = nombres_preprocesamientos[i_proc]\n",
    "                    preprocesamiento = preprocesamientos[i_proc]\n",
    "                    \n",
    "                    #Agrupar los experimentos por procesamiento para optmizar el preprocesamiento \n",
    "                    experimentos_preprocesamientos = [e for e in experimentos if e[0] == pesado\n",
    "                                                        and e[1][1] == nombre_grama\n",
    "                                                        and e[2][1] == nombre_preprocesamiento]\n",
    "                    print('Preprocesando textos...')\n",
    "                    vec, X = tokenizar(textos, pesado, n_grama, preprocesamiento, nombre_dataset)\n",
    "                    \n",
    "                    for e in experimentos_preprocesamientos:                  \n",
    "                        inicializacion = e[3][0]\n",
    "                        nombre_inicializacion = e[3][1]\n",
    "                        neuronas = e[4]\n",
    "                        learning_rate = e[5]\n",
    "                        batch = e[6]\n",
    "                        diccionario = {\n",
    "                                    'Dataset':nombre_dataset, \n",
    "                                    'Pesado':pesado,\n",
    "                                    'N-gramas':nombre_grama,\n",
    "                                    'Preprocesamiento':nombre_preprocesamiento,\n",
    "                                    'Initialization':nombre_inicializacion,\n",
    "                                    'Hidden-layer-size':neuronas,\n",
    "                                    'Learning-rate':learning_rate,\n",
    "                                    'Batch-size':batch,\n",
    "                                    'Input-size':X.shape[1],\n",
    "                                    'Output-size':salidas,\n",
    "                                    'Activation':'Sigmoid'}\n",
    "                        print('Características del modelo:')\n",
    "                        print(diccionario)\n",
    "                        modelo = MultiLayerPerceptron(num_entradas=X.shape[1], \n",
    "                                                    num_neuronas_ocultas=neuronas, \n",
    "                                                    num_salidas=salidas, \n",
    "                                                    inicialitazion_function=inicializacion, \n",
    "                                                    loss_function=loss_function_MSE,\n",
    "                                                    activation_function=sigmoid,\n",
    "                                                    activation_derivative=sigmoid_derivative,\n",
    "                                                    epochs=epochs, \n",
    "                                                    batch_size=batch, \n",
    "                                                    learning_rate=learning_rate,\n",
    "                                                    stop_error=0.001,\n",
    "                                                    rango_epocas=10,\n",
    "                                                    umbral_mejora=0.0005)\n",
    "                        modelo.train(X.toarray(), Y)\n",
    "                        print(f'{'█'*50} Resultados {'█'*50}')\n",
    "                        a, f1, pr, rc = test_modelo(modelo, vec, X_test, Y_test)\n",
    "                        diccionario.update({    \n",
    "                            'Epochs':modelo.epochs,\n",
    "                            'Accuracy':a,\n",
    "                            'F1-score':f1,\n",
    "                            'Precision':pr,\n",
    "                            'Recall':rc\n",
    "                        })\n",
    "                        guardar_modelos((diccionario, modelo), f'Modelos{nombres_datasets[i]}.pkl')\n",
    "                        graficar(diccionario, modelo)\n",
    "                        n_modelos += 1\n",
    "                        with open('DiccionarioModelos.txt', 'a') as file:\n",
    "                            file.write(f'{n_modelos} {diccionario}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Análisis de variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelos(filename, key = None, value = None):\n",
    "    file = open(filename, 'rb')\n",
    "    modelos = []\n",
    "    while 1:\n",
    "        try:\n",
    "            tupla = pickle.load(file)\n",
    "            if key is not None and value is not None:\n",
    "                if (tupla[0][key] == value):\n",
    "                    modelos.append((tupla[0], tupla[1].epochs_error))\n",
    "            else:\n",
    "                modelos.append((tupla[0], tupla[1].epochs_error))\n",
    "                \n",
    "        except EOFError:\n",
    "            break     \n",
    "    file.close()\n",
    "    return modelos\n",
    "    \n",
    "def graficar_conjunto(listas_modelos, nombres_labels, titulo, caracteristicas, filename, epocas, opacidad=0.2):\n",
    "    \"\"\"\n",
    "    Grafica un conjunto de redes neuronales en la misma grafica\n",
    "    \"\"\"\n",
    "    colores = ['black', 'darkred', 'forestgreen', 'darkviolet', 'darkorange', 'royalblue']\n",
    "    marcadores = ['solid', (0, (3, 1, 1, 1)), 'dotted', (0, (5, 1)), 'dashed', (0, (5, 10)), (5, (10, 3)), (0, (3, 5, 1, 5))]\n",
    "    i = 0\n",
    "    offset = len(colores) - len(listas_modelos)\n",
    "    patches = []\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title(titulo)\n",
    "    plt.figtext(0.13, 0.03, caracteristicas)\n",
    "    for l in listas_modelos:    \n",
    "        color = colores[i + offset]\n",
    "        for t in l:\n",
    "            epocas_modelo = len(t[1])\n",
    "            epocas_faltantes = 100 - epocas_modelo\n",
    "            plt.plot(np.arange(epocas), t[1] + [None]*epocas_faltantes, color=color, linestyle=marcadores[i], linewidth=1, alpha=opacidad)\n",
    "            if len(t[1]) < 100:\n",
    "                plt.plot(len(t[1]) - 1, t[1][-1], 'x', color=color)\n",
    "        patches.append(mpatches.Patch(color=color, label=nombres_labels[i]))\n",
    "        i += 1\n",
    "    plt.legend(handles=patches)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Filtrar por tipo de pesado\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pesado_TF = \u001b[43mcargar_modelos\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mModelosEN.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPesado\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m pesado_TF_IDF = cargar_modelos(\u001b[33m'\u001b[39m\u001b[33mModelosEN.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPesado\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTF-IDF\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m graficar_conjunto(listas_modelos=[pesado_TF, pesado_TF_IDF], \n\u001b[32m      8\u001b[39m                   nombres_labels=[\u001b[33m'\u001b[39m\u001b[33mTF\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTF-IDF\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m      9\u001b[39m                   titulo=\u001b[33m'\u001b[39m\u001b[33mError por tipo de vectorización\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                   epocas=\u001b[32m100\u001b[39m,\n\u001b[32m     13\u001b[39m                   opacidad=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcargar_modelos\u001b[39m\u001b[34m(filename, key, value)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[32m1\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         tupla = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (tupla[\u001b[32m0\u001b[39m][key] == value):\n\u001b[32m      8\u001b[39m             modelos.append((tupla[\u001b[32m0\u001b[39m], tupla[\u001b[32m1\u001b[39m].epochs_error))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Filtrar por tipo de pesado\n",
    "# --------------------------------------\n",
    "pesado_TF = cargar_modelos('ModelosEN.pkl', 'Pesado', 'TF')\n",
    "pesado_TF_IDF = cargar_modelos('ModelosEN.pkl', 'Pesado', 'TF-IDF')\n",
    "\n",
    "graficar_conjunto(listas_modelos=[pesado_TF, pesado_TF_IDF], \n",
    "                  nombres_labels=['TF', 'TF-IDF'], \n",
    "                  titulo='Error por tipo de vectorización', \n",
    "                  caracteristicas='Dataset: EN', \n",
    "                  filename='./graficas/conjuntos/Pesado.png', \n",
    "                  epocas=100,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "# --------------------------------------\n",
    "# Filtrar por tipo de representación\n",
    "# --------------------------------------\n",
    "modelos = pesado_TF + pesado_TF_IDF\n",
    "unigramas = [t for t in modelos if t[0]['N-gramas'] == 'Unigramas']\n",
    "bigramas = [t for t in modelos if t[0]['N-gramas'] == 'Bigramas']\n",
    "multigramas = [t for t in modelos if t[0]['N-gramas'] == 'Unigramas+Bigramas']\n",
    "\n",
    "graficar_conjunto(listas_modelos=[unigramas, bigramas, multigramas], \n",
    "                  nombres_labels=['Unigramas', 'Bigramas', 'Unigramas y Bigramas'], \n",
    "                  titulo='Error por tipo de representación de términos', \n",
    "                  caracteristicas='Dataset: EN', \n",
    "                  filename='./graficas/conjuntos/NGramas.png', \n",
    "                  epocas=100,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "# --------------------------------------\n",
    "# Filtrar por tipo de preprocesamiento\n",
    "# --------------------------------------\n",
    "normalizacion = [t for t in modelos if t[0]['Preprocesamiento'] == 'Normalizacion']\n",
    "norm_stopwords = [t for t in modelos if t[0]['Preprocesamiento'] == 'Normalizacion+Stopwords']\n",
    "norm_stop_stemming = [t for t in modelos if t[0]['Preprocesamiento'] == 'Normalizacion+Stopwords+Stemming']\n",
    "\n",
    "graficar_conjunto(listas_modelos=[normalizacion, norm_stopwords, norm_stop_stemming], \n",
    "                  nombres_labels=['Normalizacion', 'Normalizacion y remoción de stopwords', 'Normalizacion, remoción de stopwords y Stemming'], \n",
    "                  titulo='Error por tipo de preprocesamiento', \n",
    "                  caracteristicas='Dataset: EN', \n",
    "                  filename='./graficas/conjuntos/Preprocesamiento.png', \n",
    "                  epocas=100,\n",
    "                  opacidad=0.5)\n",
    "\n",
    "# --------------------------------------\n",
    "# Filtrar por cantidad de neuronas\n",
    "# --------------------------------------\n",
    "n_64 = [t for t in modelos if t[0]['Hidden-layer-size'] == 64]\n",
    "n_128 = [t for t in modelos if t[0]['Hidden-layer-size'] == 128]\n",
    "n_256 = [t for t in modelos if t[0]['Hidden-layer-size'] == 256]\n",
    "n_512 = [t for t in modelos if t[0]['Hidden-layer-size'] == 512]\n",
    "n_1024 = [t for t in modelos if t[0]['Hidden-layer-size'] == 1024]\n",
    "\n",
    "graficar_conjunto(listas_modelos=[n_64, n_128, n_256, n_512, n_1024], \n",
    "                  nombres_labels=['64', '128', '256', '512', '1024'], \n",
    "                  titulo='Error por tipo de preprocesamiento', \n",
    "                  caracteristicas='Dataset: EN', \n",
    "                  filename='./graficas/conjuntos/Neuronas.png', \n",
    "                  epocas=100,\n",
    "                  opacidad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mejores modelos</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>K-folds</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Obtener los 3 mejores modelos por dataset\n",
    "# -----------------------------------------\n",
    "def leer_diccionarios(filename):\n",
    "    \"\"\"\n",
    "    Lee todos los diccionarios desde el archivo de texto y los regresa\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    diccionarios = []\n",
    "    for line in file.readlines():   \n",
    "        _, d = line.split(' ', maxsplit=1)\n",
    "        diccionarios.append(eval(d[:-1]))\n",
    "    file.close()\n",
    "    return diccionarios\n",
    "\n",
    "def get_f1(d):\n",
    "    return d['F1-score']\n",
    "\n",
    "lista_diccionarios = leer_diccionarios('DiccionarioModelos.txt')\n",
    "diccionarios_EN = lista_diccionarios[:150]\n",
    "diccionarios_ES = lista_diccionarios[150:]\n",
    "diccionarios_EN.sort(key=get_f1, reverse=True)\n",
    "diccionarios_ES.sort(key=get_f1, reverse=True)\n",
    "top3_EN = diccionarios_EN[:3]\n",
    "top3_ES = diccionarios_ES[:3]\n",
    "mejores_modelos = top3_EN + top3_ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "F1: 0.732\n",
      "Precision: 0.735\n",
      "Recall: 0.740\n",
      "Accuracy: 0.733\n",
      "1\n",
      "F1: 0.728\n",
      "Precision: 0.728\n",
      "Recall: 0.729\n",
      "Accuracy: 0.734\n",
      "2\n",
      "F1: 0.726\n",
      "Precision: 0.725\n",
      "Recall: 0.726\n",
      "Accuracy: 0.731\n",
      "3\n",
      "F1: 0.789\n",
      "Precision: 0.790\n",
      "Recall: 0.788\n",
      "Accuracy: 0.792\n",
      "4\n",
      "F1: 0.785\n",
      "Precision: 0.785\n",
      "Recall: 0.785\n",
      "Accuracy: 0.788\n",
      "5\n",
      "F1: 0.785\n",
      "Precision: 0.785\n",
      "Recall: 0.785\n",
      "Accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for m in mejores_modelos:\n",
    "    print(i)\n",
    "    print(f'F1: {m['F1-score']:.3f}')\n",
    "    print(f'Precision: {m['Precision']:.3f}')\n",
    "    print(f'Recall: {m['Recall']:.3f}')\n",
    "    print(f'Accuracy: {m['Accuracy']:.3f}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENTAR:\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    n_modelos = 0\n",
    "    accuracys = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for d in mejores_modelos:\n",
    "        #Buscar los parámetros a partir de la string\n",
    "        nombre_grama = d['N-gramas']\n",
    "        nombre_preprocesamiento = d['Preprocesamiento']\n",
    "        nombre_inicializacion = d['Initialization']\n",
    "        i_grama = nombres_n_gramas.index(nombre_grama)\n",
    "        i_preprocesamiento = nombres_preprocesamientos.index(nombre_preprocesamiento)\n",
    "        i_init = nombres_inicializacion.index(nombre_inicializacion)\n",
    "        n_grama = n_gramas[i_grama]\n",
    "        preprocesamiento = preprocesamientos[i_preprocesamiento]\n",
    "        inicializacion = initialization_functions[i_init]\n",
    "\n",
    "        if d['Dataset'] == 'EN':\n",
    "            X_all = textos_en_all\n",
    "            Y_all = Y_en_all\n",
    "        elif d['Dataset'] == 'ES':\n",
    "            X_all = textos_es_all\n",
    "            Y_all = Y_es_all\n",
    "            \n",
    "        for k, (index_train, index_test) in enumerate(skf.split(X_all, Y_all), start=1):\n",
    "            X_train_fold, X_test_fold = X_all[index_train], X_all[index_test]\n",
    "            Y_train_fold, Y_test_fold = Y_all[index_train], Y_all[index_test]\n",
    "            print(f\"{'#'*50} k = {k} {'#'*50}\")\n",
    "            vec, X = tokenizar(X_train_fold, d['Pesado'], n_grama, preprocesamiento, d['Dataset'])\n",
    "            diccionario = {\n",
    "                'Dataset':d['Dataset'],\n",
    "                'Pesado':d['Pesado'],\n",
    "                'N-gramas':d['N-gramas'],\n",
    "                'Preprocesamiento':d['Preprocesamiento'],\n",
    "                'Initialization':d['Initialization'],\n",
    "                'Hidden-layer-size':d['Hidden-layer-size'],\n",
    "                'Learning-rate':d['Learning-rate'],\n",
    "                'Batch-size':d['Batch-size'],\n",
    "                'Input-size':X.shape[1],\n",
    "                'Output-size':d['Output-size'],\n",
    "                'Activation':d['Activation']\n",
    "                }\n",
    "            modelo = MultiLayerPerceptron(num_entradas=X.shape[1],\n",
    "                                        num_neuronas_ocultas=d['Hidden-layer-size'], \n",
    "                                        num_salidas=d['Output-size'], \n",
    "                                        inicialitazion_function=inicializacion, \n",
    "                                        loss_function=loss_function_MSE,\n",
    "                                        activation_function=sigmoid,\n",
    "                                        activation_derivative=sigmoid_derivative,\n",
    "                                        epochs=epochs, \n",
    "                                        batch_size=d['Batch-size'], \n",
    "                                        learning_rate=d['Learning-rate'],\n",
    "                                        stop_error=0.001,\n",
    "                                        rango_epocas=10,\n",
    "                                        umbral_mejora=0.0005\n",
    "                                        )\n",
    "            modelo.train(X.toarray(), Y_train_fold)\n",
    "            print(f'{'█'*50} Resultados {'█'*50}')\n",
    "            a, f1, pr, rc = test_modelo(modelo, vec, X_test_fold, Y_test_fold)\n",
    "            diccionario.update({\n",
    "                'Epochs':modelo.epochs,\n",
    "                'Accuracy':a,\n",
    "                'F1-score':f1,\n",
    "                'Precision':pr,\n",
    "                'Recall':rc\n",
    "            })\n",
    "            accuracys.append(a)\n",
    "            precisions.append(pr)\n",
    "            recalls.append(rc)\n",
    "            f1_scores.append(f1)\n",
    "            guardar_modelos((diccionario, modelo), f'Modelos{d['Dataset']}_{n_modelos}.pkl')\n",
    "            graficar(diccionario, modelo)\n",
    "            with open(f'Diccionarios{d['Dataset']}_{n_modelos}.txt', 'a') as file:\n",
    "                file.write(f'{n_modelos} {diccionario}\\n')\n",
    "        n_modelos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Resultados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 0 {'Dataset': 'EN', 'Pesado': 'TF-IDF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion', 'Initialization': 'Xavier', 'Hidden-layer-size': 64, 'Learning-rate': 0.1, 'Batch-size': 32, 'Input-size': 132217, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 42, 'Accuracy': 0.638, 'F1-score': 0.637628932026395, 'Precision': 0.6636009071058024, 'Recall': 0.6595202638671802}\n",
      "F1-score: 0.717871554397165\n",
      "Precision: 0.7252178092955462\n",
      "Recall: 0.7218599108135494\n",
      "Accuracy: 0.7237\n",
      "\n",
      "N: 1 {'Dataset': 'EN', 'Pesado': 'TF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion', 'Initialization': 'Xavier', 'Hidden-layer-size': 64, 'Learning-rate': 0.01, 'Batch-size': 32, 'Input-size': 132217, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 81, 'Accuracy': 0.643, 'F1-score': 0.6417529419910709, 'Precision': 0.6463942307692307, 'Recall': 0.6499021574588015}\n",
      "F1-score: 0.7121098824708614\n",
      "Precision: 0.7298494944318876\n",
      "Recall: 0.7102666978450026\n",
      "Accuracy: 0.726\n",
      "\n",
      "N: 2 {'Dataset': 'EN', 'Pesado': 'TF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion', 'Initialization': 'Xavier', 'Hidden-layer-size': 512, 'Learning-rate': 0.01, 'Batch-size': 64, 'Input-size': 132217, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 81, 'Accuracy': 0.645, 'F1-score': 0.6438017490839183, 'Precision': 0.6485800952880882, 'Recall': 0.6521154090720753}\n",
      "F1-score: 0.7142531859066539\n",
      "Precision: 0.7306958909860652\n",
      "Recall: 0.7124586169126064\n",
      "Accuracy: 0.7276\n",
      "\n",
      "N: 3 {'Dataset': 'ES', 'Pesado': 'TF-IDF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion', 'Initialization': 'Xavier', 'Hidden-layer-size': 64, 'Learning-rate': 0.1, 'Batch-size': 32, 'Input-size': 70218, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 54, 'Accuracy': 0.802, 'F1-score': 0.797317216981132, 'Precision': 0.795923604237692, 'Recall': 0.7992585727525486}\n",
      "F1-score: 0.7945761344596789\n",
      "Precision: 0.7961429618758699\n",
      "Recall: 0.7936076418423959\n",
      "Accuracy: 0.8012\n",
      "\n",
      "N: 4 {'Dataset': 'ES', 'Pesado': 'TF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion+Stopwords+Stemming', 'Initialization': 'Xavier', 'Hidden-layer-size': 512, 'Learning-rate': 0.01, 'Batch-size': 16, 'Input-size': 51327, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 100, 'Accuracy': 0.779, 'F1-score': 0.7729679864314984, 'Precision': 0.7723683563193886, 'Recall': 0.7736484399135002}\n",
      "F1-score: 0.7852351435298089\n",
      "Precision: 0.7901797788991146\n",
      "Recall: 0.782736142144976\n",
      "Accuracy: 0.7936\n",
      "\n",
      "N: 5 {'Dataset': 'ES', 'Pesado': 'TF-IDF', 'N-gramas': 'Unigramas+Bigramas', 'Preprocesamiento': 'Normalizacion', 'Initialization': 'Normal distribution', 'Hidden-layer-size': 64, 'Learning-rate': 0.1, 'Batch-size': 32, 'Input-size': 70218, 'Output-size': 1, 'Activation': 'Sigmoid', 'Epochs': 56, 'Accuracy': 0.782, 'F1-score': 0.7758225666465113, 'Precision': 0.7754591499307835, 'Recall': 0.7762125424776027}\n",
      "F1-score: 0.7847976644213763\n",
      "Precision: 0.7864011669572288\n",
      "Recall: 0.7837527824681233\n",
      "Accuracy: 0.7918000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Promediar los K-folds\n",
    "# ----------------------\n",
    "nombres = !ls | grep DiccionariosE\n",
    "accuracys = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "resultados = []\n",
    "\n",
    "for n in nombres:\n",
    "    lista_diccionarios = leer_diccionarios(n)\n",
    "    for d in lista_diccionarios:\n",
    "        accuracys.append(d['Accuracy'])\n",
    "        precisions.append(d['Precision'])\n",
    "        recalls.append(d['Recall'])\n",
    "        f1_scores.append(d['F1-score'])\n",
    "    accuracys = np.array(accuracys, dtype=np.float64)\n",
    "    a = accuracys.mean()\n",
    "    precisions = np.array(precisions, dtype=np.float64)\n",
    "    pr = precisions.mean()\n",
    "    recalls = np.array(recalls, dtype=np.float64)\n",
    "    rc = recalls.mean()\n",
    "    f1_scores = np.array(f1_scores, dtype=np.float64)\n",
    "    f1 = f1_scores.mean()\n",
    "    resultados.append((n[-5], a, pr, rc, f1))\n",
    "    print(f'N: {n[-5]} {d}')\n",
    "    print(f'F1-score: {f1}')\n",
    "    print(f'Precision: {pr}')\n",
    "    print(f'Recall: {rc}')\n",
    "    print(f'Accuracy: {a}\\n')\n",
    "    accuracys = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
