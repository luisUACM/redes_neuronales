{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- #### **Word embeddings** son representaciones numéricas densas y continuas de palabras en un espacio vectorial.\n",
    "- #### Estas representaciones capturan relaciones semánticas y sintácticas entre palabras.\n",
    "- #### Palabras con significados similares están más cercanas en el espacio vectorial.\n",
    "- #### Densidad: Cada palabra se representa como un vector en un espacio de dimensiones reducidas (por ejemplo, 100 o 300 dimensiones).\n",
    "- #### Diferente a las representaciones como las matrices dispersas en el modelo de \"bolsa de palabras\".\n",
    "- #### Similitud semántica: Las palabras con significados similares tendrán vectores cercanos en el espacio vectorial.\n",
    "- #### Por ejemplo, en un buen modelo de embeddings, los vectores de \"rey\" y \"reina\" estarán cerca.\n",
    "- #### Relaciones semánticas y aritmética vectorial:\n",
    "- #### Se pueden realizar operaciones matemáticas que reflejan relaciones semánticas, como:\n",
    "- #### **rey−hombre+mujer≈reina**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoques Word Embeddings:\n",
    "- #### los embeddings generalmente se entrenan a partir de grandes cantidades de texto utilizando algoritmos que buscan capturar las co-ocurrencias de palabras en un contexto dado. \n",
    "- #### Algunos métodos populares son:\n",
    "- #### **Word2Vec**:\n",
    "   - #### Utiliza dos enfoques: Skip-Gram (predice el contexto dada una palabra) y CBOW (predice una palabra dado su contexto).\n",
    "- #### **GloVe** (Global Vectors for Word Representation):\n",
    "    - #### Basado en una matriz de co-ocurrencia de palabras en un corpus grande.\n",
    "    - #### Intenta capturar la probabilidad relativa de dos palabras que co-ocurren.\n",
    "- #### **FastText**:\n",
    "    - #### Similar a Word2Vec, pero considera subpalabras (caracteres), lo que mejora la representación de palabras raras o con errores ortográficos.\n",
    "- #### **Contextuales (p. ej., BERT, GPT)**:\n",
    "    - #### Modelos que generan representaciones de palabras dependiendo del contexto en el que aparecen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos FastText\n",
    "\n",
    "### [https://fasttext.cc](https://fasttext.cc)\n",
    "\n",
    "## Modelo pre-entrenados para el idioma español\n",
    "\n",
    "### [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html#models)\n",
    "\n",
    "\n",
    "## Modelo pre-entrenados para diferentes regiones del idioma español\n",
    "\n",
    "### [https://ingeotec.github.io/regional-spanish-models](https://ingeotec.github.io/regional-spanish-models/#resources)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación del paquete FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages (from fasttext) (72.1.0)\n",
      "Requirement already satisfied: numpy in /home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages (from fasttext) (2.3.1)\n",
      "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp313-cp313-linux_x86_64.whl size=360324 sha256=b572317aa21fd0aa6af82dbe4bce641036ae7d9a055122912f8524e22311807f\n",
      "  Stored in directory: /home/luis-beto/.cache/pip/wheels/59/06/3f/c95dbba0df6e58ba6ed18220c1aea1e96042802139df3674ef\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [fasttext]━━\u001b[0m \u001b[32m1/2\u001b[0m [fasttext]\n",
      "\u001b[1A\u001b[2KSuccessfully installed fasttext-0.9.3 pybind11-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo pre-entrenado para la codificación de word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Descargar el modelo para el español de la página de FastText\n",
    "ft = fasttext.load_model('./MX.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el vector de una palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.18252420e-01 -2.93410331e-01 -4.63250242e-02  1.57723173e-01\n",
      "  1.33358082e-02  3.37742686e-01 -1.56898890e-02 -1.39179423e-01\n",
      "  1.27514556e-01 -2.33436618e-02 -2.78000444e-01  3.36885393e-01\n",
      "  3.71062867e-02  2.08314151e-01 -2.64740214e-02  2.66547680e-01\n",
      "  2.15816244e-01 -2.64567643e-01 -1.98068187e-01 -9.89820957e-02\n",
      " -1.45461440e-01  3.61161381e-01  2.09435105e-01 -1.72621399e-01\n",
      " -4.42510359e-02 -1.51117608e-01 -9.01566371e-02 -1.43508464e-01\n",
      " -3.32263887e-01  3.37214470e-01  2.03977153e-01  7.20374227e-01\n",
      " -3.94217670e-01  1.93130240e-01  1.21012591e-01  2.83492893e-01\n",
      "  6.00845933e-01 -8.30683112e-01 -1.12133950e-01 -2.89845198e-01\n",
      "  8.41842145e-02 -2.51568496e-01 -3.85965824e-01  1.07584395e-01\n",
      " -4.77332354e-01 -2.53418297e-01 -4.15728241e-01  1.86164230e-01\n",
      " -1.94786191e-01  9.72599238e-02 -8.31543282e-02 -4.53584865e-02\n",
      " -3.55329752e-01 -1.66799948e-01  1.80169821e-01  5.46473712e-02\n",
      "  1.15033805e-01  6.20717928e-03 -1.87568784e-01 -8.26224327e-01\n",
      " -1.24851465e-02  6.54482245e-02  5.16008675e-01 -2.71515064e-02\n",
      "  4.23365533e-01 -2.80764878e-01 -3.63939209e-04  3.52160513e-01\n",
      " -2.02147841e-01 -1.43693259e-03  9.10082757e-02  1.77339792e-01\n",
      "  4.40455854e-01  9.58058611e-02  2.41735145e-01  2.15515450e-01\n",
      " -3.85092497e-01 -4.88159806e-01 -4.62107539e-01 -2.73951709e-01\n",
      " -2.43846700e-01  7.52441138e-02 -5.48300862e-01 -2.40072861e-01\n",
      " -2.11615399e-01 -5.69418550e-01  5.92963807e-02  4.74477261e-02\n",
      " -1.55330375e-01 -4.23228219e-02 -1.10805526e-01  2.34241232e-01\n",
      " -1.96107566e-01 -1.01641178e-01  1.28239185e-01 -1.26926944e-01\n",
      "  1.98211551e-01 -8.99301395e-02 -1.98998719e-01 -4.75407690e-01\n",
      " -5.49292052e-03  2.13851288e-01 -9.80246067e-02  2.42429495e-01\n",
      "  1.85433552e-01  3.49433661e-01 -8.77119899e-02  2.01764539e-01\n",
      "  1.84080347e-01  8.91796127e-02 -5.28123736e-01  1.02376767e-01\n",
      " -2.51084387e-01  2.22321786e-02  1.10910833e-01 -1.57568872e-01\n",
      " -3.40559363e-01  5.54030277e-02 -2.52335221e-01  2.64321327e-01\n",
      " -4.60935950e-01 -7.30446726e-02 -3.32934856e-01 -1.78217236e-02\n",
      " -6.56903535e-02  1.06031984e-01 -7.35998675e-02  4.14135695e-01\n",
      "  1.15261495e-01  1.81078121e-01  1.17016666e-01 -6.37837052e-02\n",
      " -1.61191300e-01  7.97199905e-02  4.65695560e-01  3.90462428e-02\n",
      "  1.36438310e-01  7.56105334e-02  4.86797877e-02  2.80133575e-01\n",
      " -1.95112556e-01  6.20276034e-02 -2.38172457e-01 -7.74109513e-02\n",
      "  3.94217260e-02 -2.88483113e-01 -1.34301648e-01 -3.39402318e-01\n",
      "  6.74073845e-02 -1.13146894e-01 -1.19065017e-01 -6.07559271e-02\n",
      "  1.48549512e-01  5.14895558e-01  7.14034066e-02 -1.61641553e-01\n",
      "  1.14655323e-01  4.44154441e-01 -7.48984218e-02  1.42475247e-01\n",
      "  9.42147449e-02 -2.91048199e-01 -3.42015065e-02  3.73596579e-01\n",
      "  2.56823987e-01  1.47114843e-01 -5.28104641e-02  3.82378072e-01\n",
      "  3.97121996e-01 -1.55053943e-01 -1.03256188e-01  1.55185342e-01\n",
      " -1.72207475e-01  1.40076935e-01  1.23817727e-01 -2.15749200e-02\n",
      " -1.37679011e-01  2.90381461e-01 -3.07105094e-01  1.27268106e-01\n",
      "  6.38751313e-02 -2.60958552e-01 -3.50141637e-02 -9.82704982e-02\n",
      " -4.20662105e-01  1.92131549e-01 -2.16862619e-01  5.89108109e-01\n",
      " -3.02754492e-01  7.46675506e-02 -1.19844787e-01 -1.08743392e-01\n",
      "  2.29475394e-01 -1.78743199e-01 -1.44230157e-01 -3.53854448e-01\n",
      "  2.41424963e-02 -3.60809147e-01  4.43476558e-01 -3.64831984e-01\n",
      " -1.33769020e-01  2.64936745e-01  1.92404732e-01 -2.65239865e-01\n",
      "  7.66718313e-02  9.44708753e-03  2.39840478e-01 -4.49623048e-01\n",
      "  1.78111061e-01 -6.82083741e-02  2.26913422e-01 -8.08601156e-02\n",
      "  7.55792186e-02 -3.28464597e-01  2.72105694e-01 -2.10769594e-01\n",
      "  2.34254494e-01 -2.47135520e-01  6.87286025e-03 -4.78285939e-01\n",
      " -7.91866928e-02 -1.74168393e-01  3.08688879e-02 -2.73682356e-01\n",
      " -9.96523350e-02 -5.42931080e-01  5.63332401e-02  6.44381111e-03\n",
      " -3.81357819e-01 -4.66673791e-01  4.08105642e-01 -1.39513150e-01\n",
      " -5.04432797e-01 -6.18126839e-02  1.41917303e-01  4.28775012e-01\n",
      " -2.71722317e-01 -7.42273554e-02  6.68131828e-01  3.47733706e-01\n",
      " -2.07449440e-02 -6.71174452e-02 -3.09163541e-01  2.33359840e-02\n",
      "  9.83106345e-02 -5.91289587e-02 -2.27853179e-01  1.98007062e-01\n",
      " -1.23566231e-02  1.42292976e-01  2.77185202e-01 -1.43285111e-01\n",
      "  3.32920313e-01  6.22323491e-02 -2.53888909e-02 -1.88790500e-01\n",
      " -4.37233031e-01  5.17292246e-02 -1.92141488e-01  1.65743709e-01\n",
      "  1.43150166e-01  1.97211295e-01  2.70391613e-01 -4.54796791e-01\n",
      "  2.08150953e-01  2.14707851e-01  4.38289016e-01  8.30490589e-02\n",
      " -4.80412751e-01 -4.03032869e-01  7.12093785e-02  1.02208614e-01\n",
      "  2.69002974e-01 -2.90724367e-01 -5.36856830e-01 -3.41138065e-01\n",
      "  7.87762552e-02 -1.88576683e-01  2.26396292e-01 -7.53493428e-01\n",
      " -2.60188188e-02  2.41447449e-01  3.46010000e-01  6.79264069e-02\n",
      "  1.30126610e-01  3.22751135e-01  9.77263823e-02 -6.22871369e-02\n",
      "  5.32856099e-02 -1.06469663e-02 -3.90205085e-01 -2.64374495e-01\n",
      " -1.36684611e-01  8.86090919e-02 -1.17529392e-01  3.66933405e-01\n",
      "  6.94891578e-03  1.70478001e-01  1.90143809e-01  2.74161279e-01]\n",
      "[ 2.18252420e-01 -2.93410331e-01 -4.63250242e-02  1.57723173e-01\n",
      "  1.33358082e-02  3.37742686e-01 -1.56898890e-02 -1.39179423e-01\n",
      "  1.27514556e-01 -2.33436618e-02 -2.78000444e-01  3.36885393e-01\n",
      "  3.71062867e-02  2.08314151e-01 -2.64740214e-02  2.66547680e-01\n",
      "  2.15816244e-01 -2.64567643e-01 -1.98068187e-01 -9.89820957e-02\n",
      " -1.45461440e-01  3.61161381e-01  2.09435105e-01 -1.72621399e-01\n",
      " -4.42510359e-02 -1.51117608e-01 -9.01566371e-02 -1.43508464e-01\n",
      " -3.32263887e-01  3.37214470e-01  2.03977153e-01  7.20374227e-01\n",
      " -3.94217670e-01  1.93130240e-01  1.21012591e-01  2.83492893e-01\n",
      "  6.00845933e-01 -8.30683112e-01 -1.12133950e-01 -2.89845198e-01\n",
      "  8.41842145e-02 -2.51568496e-01 -3.85965824e-01  1.07584395e-01\n",
      " -4.77332354e-01 -2.53418297e-01 -4.15728241e-01  1.86164230e-01\n",
      " -1.94786191e-01  9.72599238e-02 -8.31543282e-02 -4.53584865e-02\n",
      " -3.55329752e-01 -1.66799948e-01  1.80169821e-01  5.46473712e-02\n",
      "  1.15033805e-01  6.20717928e-03 -1.87568784e-01 -8.26224327e-01\n",
      " -1.24851465e-02  6.54482245e-02  5.16008675e-01 -2.71515064e-02\n",
      "  4.23365533e-01 -2.80764878e-01 -3.63939209e-04  3.52160513e-01\n",
      " -2.02147841e-01 -1.43693259e-03  9.10082757e-02  1.77339792e-01\n",
      "  4.40455854e-01  9.58058611e-02  2.41735145e-01  2.15515450e-01\n",
      " -3.85092497e-01 -4.88159806e-01 -4.62107539e-01 -2.73951709e-01\n",
      " -2.43846700e-01  7.52441138e-02 -5.48300862e-01 -2.40072861e-01\n",
      " -2.11615399e-01 -5.69418550e-01  5.92963807e-02  4.74477261e-02\n",
      " -1.55330375e-01 -4.23228219e-02 -1.10805526e-01  2.34241232e-01\n",
      " -1.96107566e-01 -1.01641178e-01  1.28239185e-01 -1.26926944e-01\n",
      "  1.98211551e-01 -8.99301395e-02 -1.98998719e-01 -4.75407690e-01\n",
      " -5.49292052e-03  2.13851288e-01 -9.80246067e-02  2.42429495e-01\n",
      "  1.85433552e-01  3.49433661e-01 -8.77119899e-02  2.01764539e-01\n",
      "  1.84080347e-01  8.91796127e-02 -5.28123736e-01  1.02376767e-01\n",
      " -2.51084387e-01  2.22321786e-02  1.10910833e-01 -1.57568872e-01\n",
      " -3.40559363e-01  5.54030277e-02 -2.52335221e-01  2.64321327e-01\n",
      " -4.60935950e-01 -7.30446726e-02 -3.32934856e-01 -1.78217236e-02\n",
      " -6.56903535e-02  1.06031984e-01 -7.35998675e-02  4.14135695e-01\n",
      "  1.15261495e-01  1.81078121e-01  1.17016666e-01 -6.37837052e-02\n",
      " -1.61191300e-01  7.97199905e-02  4.65695560e-01  3.90462428e-02\n",
      "  1.36438310e-01  7.56105334e-02  4.86797877e-02  2.80133575e-01\n",
      " -1.95112556e-01  6.20276034e-02 -2.38172457e-01 -7.74109513e-02\n",
      "  3.94217260e-02 -2.88483113e-01 -1.34301648e-01 -3.39402318e-01\n",
      "  6.74073845e-02 -1.13146894e-01 -1.19065017e-01 -6.07559271e-02\n",
      "  1.48549512e-01  5.14895558e-01  7.14034066e-02 -1.61641553e-01\n",
      "  1.14655323e-01  4.44154441e-01 -7.48984218e-02  1.42475247e-01\n",
      "  9.42147449e-02 -2.91048199e-01 -3.42015065e-02  3.73596579e-01\n",
      "  2.56823987e-01  1.47114843e-01 -5.28104641e-02  3.82378072e-01\n",
      "  3.97121996e-01 -1.55053943e-01 -1.03256188e-01  1.55185342e-01\n",
      " -1.72207475e-01  1.40076935e-01  1.23817727e-01 -2.15749200e-02\n",
      " -1.37679011e-01  2.90381461e-01 -3.07105094e-01  1.27268106e-01\n",
      "  6.38751313e-02 -2.60958552e-01 -3.50141637e-02 -9.82704982e-02\n",
      " -4.20662105e-01  1.92131549e-01 -2.16862619e-01  5.89108109e-01\n",
      " -3.02754492e-01  7.46675506e-02 -1.19844787e-01 -1.08743392e-01\n",
      "  2.29475394e-01 -1.78743199e-01 -1.44230157e-01 -3.53854448e-01\n",
      "  2.41424963e-02 -3.60809147e-01  4.43476558e-01 -3.64831984e-01\n",
      " -1.33769020e-01  2.64936745e-01  1.92404732e-01 -2.65239865e-01\n",
      "  7.66718313e-02  9.44708753e-03  2.39840478e-01 -4.49623048e-01\n",
      "  1.78111061e-01 -6.82083741e-02  2.26913422e-01 -8.08601156e-02\n",
      "  7.55792186e-02 -3.28464597e-01  2.72105694e-01 -2.10769594e-01\n",
      "  2.34254494e-01 -2.47135520e-01  6.87286025e-03 -4.78285939e-01\n",
      " -7.91866928e-02 -1.74168393e-01  3.08688879e-02 -2.73682356e-01\n",
      " -9.96523350e-02 -5.42931080e-01  5.63332401e-02  6.44381111e-03\n",
      " -3.81357819e-01 -4.66673791e-01  4.08105642e-01 -1.39513150e-01\n",
      " -5.04432797e-01 -6.18126839e-02  1.41917303e-01  4.28775012e-01\n",
      " -2.71722317e-01 -7.42273554e-02  6.68131828e-01  3.47733706e-01\n",
      " -2.07449440e-02 -6.71174452e-02 -3.09163541e-01  2.33359840e-02\n",
      "  9.83106345e-02 -5.91289587e-02 -2.27853179e-01  1.98007062e-01\n",
      " -1.23566231e-02  1.42292976e-01  2.77185202e-01 -1.43285111e-01\n",
      "  3.32920313e-01  6.22323491e-02 -2.53888909e-02 -1.88790500e-01\n",
      " -4.37233031e-01  5.17292246e-02 -1.92141488e-01  1.65743709e-01\n",
      "  1.43150166e-01  1.97211295e-01  2.70391613e-01 -4.54796791e-01\n",
      "  2.08150953e-01  2.14707851e-01  4.38289016e-01  8.30490589e-02\n",
      " -4.80412751e-01 -4.03032869e-01  7.12093785e-02  1.02208614e-01\n",
      "  2.69002974e-01 -2.90724367e-01 -5.36856830e-01 -3.41138065e-01\n",
      "  7.87762552e-02 -1.88576683e-01  2.26396292e-01 -7.53493428e-01\n",
      " -2.60188188e-02  2.41447449e-01  3.46010000e-01  6.79264069e-02\n",
      "  1.30126610e-01  3.22751135e-01  9.77263823e-02 -6.22871369e-02\n",
      "  5.32856099e-02 -1.06469663e-02 -3.90205085e-01 -2.64374495e-01\n",
      " -1.36684611e-01  8.86090919e-02 -1.17529392e-01  3.66933405e-01\n",
      "  6.94891578e-03  1.70478001e-01  1.90143809e-01  2.74161279e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Obtención del vector de una palabra de palabras\n",
    "print(ft.get_word_vector(\"hola\"))\n",
    "\n",
    "# equivalente \n",
    "# Vector Denso de la palabra \"hola\"\n",
    "print(ft[\"hola\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total de palabras en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de palabras:  438136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['</s>', '_usr', 'que', 'de', ',', '.', 'y', 'a', 'la', 'no']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtiene la lista total de palabras del modelo\n",
    "# ft.get_words()\n",
    "\n",
    "# Equivalente a la propiedad words\n",
    "\n",
    "# Obtención el total del vocabulario\n",
    "print(\"total de palabras: \", len(ft.words))\n",
    "\n",
    "#primeras 10 palabras del vocabulario\n",
    "ft.words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar oraciones en su forma de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la representación de embedding de la oración\n",
    "\n",
    "vec = ft.get_sentence_vector(\"hola me siento muy feliz\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de las palabras vecinas más cercanas basadas en vectores densos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors(\"mareado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= (ft.get_word_vector(\"rey\") - ft.get_word_vector(\"hombre\")) + ft.get_word_vector(\"mujer\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos: Analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogia = ft.get_analogies(\"rey\",\"hombre\", \"mujer\")\n",
    "print(analogia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando vectores semánticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el vocabulario de los vectores pre-entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ft.get_words(on_unicode_error=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words), type(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el word embeddings para cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "frame_words= pd.DataFrame({\"word\": words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words[\"word_embed\"] = frame_words[\"word\"].map(lambda x: ft.get_sentence_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construye la matriz de vectores densos para todo el vocabulario y poder hacer comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_VECS = np.vstack(frame_words[\"word_embed\"].to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VECS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compara semánticamente el vector X contra todo el vocabulario pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "res = cosine_similarity([X], _VECS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El valor resultante es una matriz de 1 elemento contra todos los elementos del vocabulario\n",
    "print(res.shape)\n",
    "print(res.ndim)\n",
    "# cada elemento es el resultado de la similitud coseno entre X y el elemento en esa posición)\n",
    "# se muestran los primeros 5 elementos de la comparación\n",
    "print(res[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer los indices de los valores máximos para la similitud coseno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena ascendentemente los resultados de acuerdo a las similitud obtenida\n",
    "indx = np.argsort(res[0])[-10:]\n",
    "print(indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer las palabras asociadas a los vectores más similares al vector X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indx[::-1]:\n",
    "    print(frame_words.word.loc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
