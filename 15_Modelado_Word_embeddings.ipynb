{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- #### **Word embeddings** son representaciones numéricas densas y continuas de palabras en un espacio vectorial.\n",
    "- #### Estas representaciones capturan relaciones semánticas y sintácticas entre palabras.\n",
    "- #### Palabras con significados similares están más cercanas en el espacio vectorial.\n",
    "- #### Densidad: Cada palabra se representa como un vector en un espacio de dimensiones reducidas (por ejemplo, 100 o 300 dimensiones).\n",
    "- #### Diferente a las representaciones como las matrices dispersas en el modelo de \"bolsa de palabras\".\n",
    "- #### Similitud semántica: Las palabras con significados similares tendrán vectores cercanos en el espacio vectorial.\n",
    "- #### Por ejemplo, en un buen modelo de embeddings, los vectores de \"rey\" y \"reina\" estarán cerca.\n",
    "- #### Relaciones semánticas y aritmética vectorial:\n",
    "- #### Se pueden realizar operaciones matemáticas que reflejan relaciones semánticas, como:\n",
    "- #### **rey−hombre+mujer≈reina**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoques Word Embeddings:\n",
    "- #### los embeddings generalmente se entrenan a partir de grandes cantidades de texto utilizando algoritmos que buscan capturar las co-ocurrencias de palabras en un contexto dado. \n",
    "- #### Algunos métodos populares son:\n",
    "- #### **Word2Vec**:\n",
    "   - #### Utiliza dos enfoques: Skip-Gram (predice el contexto dada una palabra) y CBOW (predice una palabra dado su contexto).\n",
    "- #### **GloVe** (Global Vectors for Word Representation):\n",
    "    - #### Basado en una matriz de co-ocurrencia de palabras en un corpus grande.\n",
    "    - #### Intenta capturar la probabilidad relativa de dos palabras que co-ocurren.\n",
    "- #### **FastText**:\n",
    "    - #### Similar a Word2Vec, pero considera subpalabras (caracteres), lo que mejora la representación de palabras raras o con errores ortográficos.\n",
    "- #### **Contextuales (p. ej., BERT, GPT)**:\n",
    "    - #### Modelos que generan representaciones de palabras dependiendo del contexto en el que aparecen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos FastText\n",
    "\n",
    "### [https://fasttext.cc](https://fasttext.cc)\n",
    "\n",
    "## Modelo pre-entrenados para el idioma español\n",
    "\n",
    "### [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html#models)\n",
    "\n",
    "\n",
    "## Modelo pre-entrenados para diferentes regiones del idioma español\n",
    "\n",
    "### [https://ingeotec.github.io/regional-spanish-models](https://ingeotec.github.io/regional-spanish-models/#resources)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación del paquete FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo pre-entrenado para la codificación de word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Descargar el modelo para el español de la página de FastText\n",
    "ft = fasttext.load_model('/Volumes/data/temp/cc.es.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el vector de una palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Obtención del vector de una palabra de palabras\n",
    "print(ft.get_word_vector(\"hola\"))\n",
    "\n",
    "# equivalente \n",
    "# Vector Denso de la palabra \"hola\"\n",
    "print(ft[\"hola\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total de palabras en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la lista total de palabras del modelo\n",
    "# ft.get_words()\n",
    "\n",
    "# Equivalente a la propiedad words\n",
    "\n",
    "# Obtención el total del vocabulario\n",
    "print(\"total de palabras: \", len(ft.words))\n",
    "\n",
    "#primeras 10 palabras del vocabulario\n",
    "ft.words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar oraciones en su forma de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la representación de embedding de la oración\n",
    "\n",
    "vec = ft.get_sentence_vector(\"hola me siento muy feliz\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de las palabras vecinas más cercanas basadas en vectores densos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors(\"mareado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= (ft.get_word_vector(\"rey\") - ft.get_word_vector(\"hombre\")) + ft.get_word_vector(\"mujer\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos: Analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogia = ft.get_analogies(\"rey\",\"hombre\", \"mujer\")\n",
    "print(analogia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando vectores semánticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el vocabulario de los vectores pre-entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ft.get_words(on_unicode_error=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words), type(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el word embeddings para cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "frame_words= pd.DataFrame({\"word\": words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words[\"word_embed\"] = frame_words[\"word\"].map(lambda x: ft.get_sentence_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construye la matriz de vectores densos para todo el vocabulario y poder hacer comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_VECS = np.vstack(frame_words[\"word_embed\"].to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VECS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compara semánticamente el vector X contra todo el vocabulario pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "res = cosine_similarity([X], _VECS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El valor resultante es una matriz de 1 elemento contra todos los elementos del vocabulario\n",
    "print(res.shape)\n",
    "print(res.ndim)\n",
    "# cada elemento es el resultado de la similitud coseno entre X y el elemento en esa posición)\n",
    "# se muestran los primeros 5 elementos de la comparación\n",
    "print(res[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer los indices de los valores máximos para la similitud coseno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena ascendentemente los resultados de acuerdo a las similitud obtenida\n",
    "indx = np.argsort(res[0])[-10:]\n",
    "print(indx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer las palabras asociadas a los vectores más similares al vector X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indx[::-1]:\n",
    "    print(frame_words.word.loc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
