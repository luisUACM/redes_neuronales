{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci贸n de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar al clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Red Neuronal Multicapa\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"600\" style=\"background-color:white;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracci贸n de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracci贸n de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Codificar las categor铆as (clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:\n",
      "['aggressive' 'nonaggressive']\n",
      "Clases codificadas:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificaci贸n ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparar los conjuntos de datos  (datasets) para entrenamiento y para probar el rendimiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Crear Matriz Documento-T茅rmino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulario:  10609\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar m谩s palabras a esta lista si es necesario\n",
    "\n",
    "# Normalizaci贸n del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]驴?隆!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuaci贸n, True deja intacta la puntuaci贸n)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los n煤meros, True deja intactos los acentos)\n",
    "        max_dup=2 (n煤mero m谩ximo de s铆mbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a min煤sculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor sem谩ntico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"despu茅s:\",texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Crear el clasificador: Crear la clase MLP_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funci贸n de activaci贸n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generaci贸n de n煤meros aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # Calcular el l铆mite de la distribuci贸n uniforme\n",
    "    limit = np.sqrt(6 / (input_size + output_size))\n",
    "    # Generar la matriz de pesos con distribuci贸n uniforme en el rango [-limit, limit]\n",
    "    W = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "    return W\n",
    "\n",
    "\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los 铆ndices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y seg煤n los 铆ndices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=32, learning_rate=0.5, random_state=42):\n",
    "\n",
    "        seed(random_state)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el n煤mero de 茅pocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tama帽o del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # definir las capas\n",
    "        self.W1 = xavier_initialization(num_neuronas_ocultas, num_entradas)  # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = xavier_initialization(num_salidas, num_neuronas_ocultas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagaci贸n hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = 0\n",
    "        # TODO: Calcular la activaci贸n de la capa oculta usando la funci贸n sigmoide\n",
    "        self.a_c1 = 0\n",
    "        # TODO: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2 = 0\n",
    "        # TODO: Calcular la activaci贸n de la capa de salida usando la funci贸n sigmoide\n",
    "        y_pred = 0  # Activaci贸n capa de salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. C谩lculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular el error cuadr谩tico medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        # TODO: implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagaci贸n hacia atr谩s (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = 0 # Derivada del error respecto a la predicci贸n con  N ejemplos\n",
    "        # TODO: Calcular la derivada de la activaci贸n de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = 0\n",
    "        # TODO: Calcular delta de la capa de salida\n",
    "        delta_c2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = 0\n",
    "        # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = 0\n",
    "\n",
    "        #calcula el gradiente de la funci贸n de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = 0\n",
    "        self.dE_db2 =0\n",
    "        self.dE_dW1 =  0\n",
    "        self.dE_db1 =  0\n",
    "\n",
    "\n",
    "    def update(self):  # Ejecuci贸n de la actualizaci贸n de param谩metros\n",
    "        # TODO: implementar la actualizaci贸n de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualizaci贸n de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = 0\n",
    "        self.b2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzaci贸n de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la funci贸n de error respecto a los pesos de la capa 1\n",
    "        self.W1 = 0\n",
    "        self.b1 = 0\n",
    "\n",
    "    def predict(self, X):  # Predecir la categor铆a para datos nuevos\n",
    "        # TODO: implementar la predicci贸n \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "        for epoch in range(self.epochs):\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                self.backward() # c谩lculo de los gradientes\n",
    "                self.update() # actualizaci贸n de los pesos y bias\n",
    "\n",
    "                # Imprimir el error cada N 茅pocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"poca {epoch}, Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Entrenar la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.421875\n",
      "poca 0, Error: 0.453125\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.28125\n",
      "poca 0, Error: 0.265625\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.4375\n",
      "poca 0, Error: 0.234375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.234375\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.25\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.4375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.28125\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.421875\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.28125\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.421875\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.421875\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.453125\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.328125\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.375\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.40625\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.296875\n",
      "poca 0, Error: 0.390625\n",
      "poca 0, Error: 0.34375\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.3125\n",
      "poca 0, Error: 0.359375\n",
      "poca 0, Error: 0.421875\n",
      "poca 0, Error: 0.2903225806451613\n",
      "poca 0, Error: 0.2903225806451613\n",
      "poca 0, Error: 0.3870967741935484\n",
      "poca 0, Error: 0.3548387096774194\n",
      "poca 0, Error: 0.3225806451612903\n",
      "poca 0, Error: 0.3387096774193548\n",
      "poca 0, Error: 0.3870967741935484\n",
      "poca 0, Error: 0.3387096774193548\n",
      "poca 0, Error: 0.27419354838709675\n",
      "poca 0, Error: 0.3709677419354839\n",
      "poca 0, Error: 0.4032258064516129\n",
      "poca 0, Error: 0.3709677419354839\n",
      "poca 0, Error: 0.3870967741935484\n",
      "poca 0, Error: 0.3225806451612903\n",
      "poca 0, Error: 0.4032258064516129\n",
      "poca 0, Error: 0.3225806451612903\n",
      "poca 0, Error: 0.2903225806451613\n",
      "poca 0, Error: 0.3709677419354839\n",
      "poca 0, Error: 0.3387096774193548\n",
      "poca 0, Error: 0.3225806451612903\n",
      "poca 0, Error: 0.3548387096774194\n",
      "poca 0, Error: 0.3387096774193548\n",
      "poca 0, Error: 0.3870967741935484\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.25\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.40625\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.453125\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.40625\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.4375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.28125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.25\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.296875\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.234375\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.265625\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.265625\n",
      "poca 100, Error: 0.40625\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.421875\n",
      "poca 100, Error: 0.390625\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.3125\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.296875\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.359375\n",
      "poca 100, Error: 0.296875\n",
      "poca 100, Error: 0.375\n",
      "poca 100, Error: 0.40625\n",
      "poca 100, Error: 0.34375\n",
      "poca 100, Error: 0.328125\n",
      "poca 100, Error: 0.45161290322580644\n",
      "poca 100, Error: 0.3709677419354839\n",
      "poca 100, Error: 0.3548387096774194\n",
      "poca 100, Error: 0.3225806451612903\n",
      "poca 100, Error: 0.3870967741935484\n",
      "poca 100, Error: 0.3548387096774194\n",
      "poca 100, Error: 0.4032258064516129\n",
      "poca 100, Error: 0.3548387096774194\n",
      "poca 100, Error: 0.3387096774193548\n",
      "poca 100, Error: 0.3225806451612903\n",
      "poca 100, Error: 0.3387096774193548\n",
      "poca 100, Error: 0.3387096774193548\n",
      "poca 100, Error: 0.3870967741935484\n",
      "poca 100, Error: 0.3225806451612903\n",
      "poca 100, Error: 0.4032258064516129\n",
      "poca 100, Error: 0.3870967741935484\n",
      "poca 100, Error: 0.3387096774193548\n",
      "poca 100, Error: 0.25806451612903225\n",
      "poca 100, Error: 0.3548387096774194\n",
      "poca 100, Error: 0.3709677419354839\n",
      "poca 100, Error: 0.41935483870967744\n",
      "poca 100, Error: 0.3870967741935484\n",
      "poca 100, Error: 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_tr = X_tfidf.toarray()\n",
    "Y_tr = Y_train[:, np.newaxis] # Agregar una dimensi贸n adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "num_entradas= X_tr.shape[1] # tama帽o de la matriz Documento-T茅rmino\n",
    "num_neuronas_ocultas = 128\n",
    "num_salidas = 1\n",
    "epochs = 110 \n",
    "batch_size = 32\n",
    "learning_rate = 0.5\n",
    "random_state = 33\n",
    "\n",
    "clasificador_mlp = MLP_TODO(num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size, learning_rate, random_state)\n",
    "\n",
    "# Entrenamos al clasificador\n",
    "clasificador_mlp.train(X_tr, Y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicci贸n de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "['aggressive']\n"
     ]
    }
   ],
   "source": [
    "ejemplos_nuevos = [\"Que se puede esperar de este perro ladr贸n\"]\n",
    "# Suponer que se cuenta con el objeto vec_tfidf entrenado con el vocabulario del conjunto de entrenamiento\n",
    "X_ejemplos_tfidf = vec_tfidf.transform(ejemplos_nuevos)\n",
    "X_ejemplos_tfidf = X_ejemplos_tfidf.toarray()\n",
    "print(X_ejemplos_tfidf)\n",
    "\n",
    "y_pred_nuevo = clasificador_mlp.predict(X_ejemplos_tfidf)\n",
    "y_pred_nuevo = y_pred_nuevo.flatten()\n",
    "print(le.inverse_transform(y_pred_nuevo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predecir los datos del conjuntos de prueba con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "X_t = X_test_tfidf.toarray()\n",
    "Y_t = Y_test[:, np.newaxis] # Agregar una dimensi贸n adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "y_pred_test = clasificador_mlp.predict(X_t)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecci贸n de los resultados de los primeros N ejemplos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textos:  ['@USUARIO @USUARIO Callate el hocico puta, ya quisieras ser mexicana Puta paname帽a de mierda tu pais es un asco igual que las personas que viven ahi puro puto negro de mierda que asco y luego son bien putos llorones ojala el bolillo se muera junto con toda la puta seleccion paname帽a aquerosa alv!!!'\n",
      " 'Twitter siempre saca mi lado filos贸fico aunque est谩 de la verga pero lo saca  #MartesDeGanarSeguidores'\n",
      " 'Comportandonos de la peor manera Dandole la espalda ala madre naturaleza Todos quieren dominar al mundo'\n",
      " 'Yo bien tonta gaste en suscribirme al canal + de BTS en V live pensando que ten铆a que hacerlo para ver el comeback'\n",
      " 'CMPRENME DULCES HIJOS DE SUS PUTAS MADRES como mantra del d铆a.']\n",
      "clase esperada:  [0 1 1 1 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtextos: \u001b[39m\u001b[33m\"\u001b[39m, X_test[:\u001b[32m5\u001b[39m])\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mclase esperada: \u001b[39m\u001b[33m\"\u001b[39m,Y_test[:\u001b[32m5\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mclase predicha: \u001b[39m\u001b[33m\"\u001b[39m, \u001b[43my_pred_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "print(\"textos: \", X_test[:5])\n",
    "print(\"clase esperada: \",Y_test[:5])\n",
    "print(\"clase predicha: \", y_pred_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar la predicci贸n de la clase original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llevar a la misma forma la salida de las predicciones\n",
    "y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obten las primeras N predicciones\n",
    "pred =  y_pred_test[:5] \n",
    "pred_ori = le.inverse_transform(pred)\n",
    "pred, pred_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluando el desempe帽o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M茅tricas de Evaluaci贸n\n",
    " - #### Las m茅tricas precisi贸n, recall y F1 son fundamentales para evaluar el rendimiento de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_precision-recall.png\" width=\"300\">\n",
    "\n",
    "##### Fuente: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_matriz-confusion.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP=True Positive\n",
    "\n",
    "TN=True Negative\n",
    "\n",
    "FP=False Positive (Error tipo I: ejemplo, se considera que el paciente est谩 enfermo, pero en realidad est谩 sano)\n",
    "\n",
    "FN=False Negative ( Error tipo II: ejemplo, se considera que el paciente est谩 sano, pero en realidad est谩 enfermo)\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{total~ TP + total~TN}{total~muestras} $$\n",
    "\n",
    "$$ Precision_c = \\frac{ TP_c}{TP_c + FP_c} $$\n",
    "\n",
    "$$ Recall_c = \\frac{ TP_c}{TP_c + FN_c} $$\n",
    "\n",
    "$$ F1-score_c= 2 \\times \\frac{ Precision_c \\times Recall_c}{Precision_c + Recall_c} $$\n",
    "\n",
    "$$ macro-F1-score= \\frac{ 1 }{|Clases|} \\sum{F1-score_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusi贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la clase 0, la precisi贸n es la siguiente\n",
    "tp= 82\n",
    "fp = 31+11+33\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M茅tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print(\"P=\", precision_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"R=\", recall_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"F1=\", f1_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"Acc=\", accuracy_score(Y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecci贸n del desempe帽o por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
