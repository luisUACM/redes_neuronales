{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "\n",
    "1. **Definir los preprocesamientos para el texto**:  \n",
    "   - convertir a minúsculas\n",
    "   - normalizar el texto: borrar símbolos, puntuación, caracteres duplicados, etc.\n",
    "\n",
    "2. **Separar los datos para entrenamiento y prueba**:  \n",
    "   - Crear los dataset de entrenamiento y test con al función train_test_split \n",
    "\n",
    "3. **Construir la matriz de Documento-Término**:  \n",
    "   - Definir los parámetros para usar unigramas\n",
    "   - Usar la clase TfidfVectorizer para construir la matriz con los datos de entrenamiento\n",
    "   \n",
    "4. **Preparar los lotes de datos (minibatches) para el entrenamiento de la red**:  \n",
    "   - Definir los minibatches con la matriz TFIDF construida\n",
    "\n",
    "5. **Definir la arquitectura de la red**:  \n",
    "   - Definir una red de 2 capas, con funciones PReLU en las capas ocultas y una capa de salida\n",
    "\n",
    "6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los datos y minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "#cargar los datos\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Definir las funciones de preprocesamiento de texto vinculadas al proceso de creación de la matriz \n",
    "# Documeno-Término creada con TfidfVectorizer.\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"después:\",texto)\n",
    "    return texto\n",
    "\n",
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "# TODO: Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "# Divide el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Crear la matriz Documento-Término con el dataset de entrenamiento: tfidfVectorizer\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Codificación de la salida onehot\n",
    "\n",
    "Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "Y_test_one_hot = nn.functional.one_hot(torch.from_numpy(Y_test), num_classes=NUM_CLASSES).float()\n",
    "Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n",
    "\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_train_tfidf = X_train_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "# Tranforma los datos de validación al espacio de representación del entrenamiento\n",
    "X_val_tfidf = vec_tfidf.transform(X_val)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_val_tfidf = X_val_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Y_train_one_hot.shape, Y_test_one_hot.shape,  Y_val_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 128\n",
    "        input_size_h2 = 8 \n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act1= nn.PReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act2= nn.PReLU()\n",
    "\n",
    "        self.output = nn.Linear(input_size_h2, output_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)        \n",
    "        if self.output.bias is not None:\n",
    "            nn.init.zeros_(self.output.bias)        \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        x = self.fc1(X)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        # Nota la última capa de salida 'output' no se activa debido a que CrossEntropyLoss usa LogSoftmax internamente. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Establecer los parámetros de la red\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size =  X_train_tfidf.shape[1]\n",
    "\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "epochs = 10 # variar el número de épocas, para probar que funciona la programación \n",
    "                 # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.01 # Generalmente se usan learning rate pequeños (0.001), \n",
    "\n",
    "# Se recomiendan tamaños de batch_size potencias de 2: 16, 32, 64, 128, 256\n",
    "# Entre mayor el número más cantidad de memoria se requiere para el procesamiento\n",
    "batch_size = 128 # definir el tamaño del lote de procesamiento \n",
    "\n",
    "\n",
    "# TODO: Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train_tfidf)\n",
    "Y_train_t = Y_train_one_hot\n",
    "\n",
    "X_val_t = torch.from_numpy(X_val_tfidf)\n",
    "\n",
    "# Crear la red\n",
    "model = MLP(input_size, output_size)\n",
    "\n",
    "# Definir la función de pérdida\n",
    "# Mean Square Error (MSE)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.BCELoss() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Definir el optimizador\n",
    "#Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Stochastic Gradient Descent (SGD)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        # inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tr)  #invoca al método forward de la clase MLP\n",
    "        # Calcular el error MSE\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        #Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        #  W = W - learning_rate * dE/dW\n",
    "        optimizer.step()\n",
    "        if np.random.random() < 0.1:\n",
    "            print(f\"Batch Error : {loss.item()}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "    \n",
    "    # Evalúa el modelo con el conjunto de validación\n",
    "    model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "    with torch.no_grad():  # No  calcular gradientes \n",
    "        y_pred = model(X_val_t)\n",
    "        # Aplica softmax para obtener las probabilidades en la evaluación\n",
    "        y_pred = torch.softmax(y_pred, dim=1)\n",
    "        # Obtiene una única clase, la más probable\n",
    "        y_pred = torch.argmax(y_pred, dim=1)        \n",
    "        print(f\"Época {epoch+1}/{epochs}\")\n",
    "        print(\"P=\", precision_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"R=\", recall_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Transformar el dataset de test con los mismos preprocesamientos y al  espacio de \n",
    "# representación vectorial que el modelo entrenado, es decir, al espacio de la matriz TFIDF\n",
    "\n",
    "# Convertir los datos de prueba a tensores de PyTorch\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_test_tfidf = X_test_tfidf.toarray().astype(np.float32)\n",
    "X_t = torch.from_numpy(X_test_tfidf)\n",
    "\n",
    "# Desactivar el comportamiento de modo de  entrenamiento: por ejemplo, capas como Dropout\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred_test= model(X_t)\n",
    "\n",
    "# y_test_pred contiene las predicciones\n",
    "\n",
    "# Obtener la clase real\n",
    "y_pred_test = torch.argmax(y_pred_test, dim=1)\n",
    "\n",
    "print(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred_test))\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "x_new_data = [\"Ese perro me robo mis cosas\", \"ese hdp se llevo el dinero\", \"mi app de calendario no sirve\"]\n",
    "x_new_data_tfidf = vec_tfidf.transform(x_new_data)\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "x_new_data_tfidf = x_new_data_tfidf.toarray().astype(np.float32)\n",
    "X_new_t = torch.from_numpy(x_new_data_tfidf)\n",
    "\n",
    "\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred = model(X_new_t)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print(le.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio.  Modificar la red neuronal MLP con las siguientes características:\n",
    "- ## Arquitectura:\n",
    "    - ### 4 capas ocultas\n",
    "    - ### 2 salidas\n",
    "    - ### Funciones de activación en capas ocultas ELU\n",
    "    - ### Número de neuronas por capa oculta a su consideración\n",
    "- ## Prepocesamiento:\n",
    "    - ### Normalización\n",
    "    - ### Repesentación de características: unigramas sin STOPWORDS y con stemming\n",
    "    - ### Pesado TF-IDF\n",
    "\n",
    "- ## Evaluación del rendimiento del modelo: \n",
    "    - ### 1. Paticiones train (80%), test (20%), validación (10% del train)\n",
    "    - ### 2. Validación cruzada k-folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from os import putenv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"10.3.0\")\n",
    "\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")\n",
    "RANDOM_STATATE = 42\n",
    "torch.manual_seed(RANDOM_STATATE)\n",
    "np.random.seed(RANDOM_STATATE)\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_tokenizador(texto):\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    texto = [stemmer.stem(t) for t in texto.split() if t not in _STOPWORDS]\n",
    "    return texto\n",
    "\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPELU4L(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super().__init__()\n",
    "        self.fcl = nn.ModuleList()\n",
    "        self.act = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 2):\n",
    "            self.fcl.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        for i in range(len(sizes) - 2):\n",
    "            nn.init.xavier_uniform_(self.fcl[i].weight)\n",
    "            nn.init.zeros_(self.fcl[i].bias)\n",
    "            self.act.append(nn.PReLU())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        for i in range(len(self.fcl)):\n",
    "            x = self.fcl[i](x)\n",
    "            x = self.act[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar(X, Y, tipo_vec='TF-IDF', test_size=0.2, val_size=0.1, ngram_range=(1,1)):\n",
    "    vec = TfidfVectorizer(analyzer=\"word\", preprocessor=normaliza_texto, tokenizer=mi_tokenizador, ngram_range=ngram_range)\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    Y_vec = le.fit_transform(Y)\n",
    "    X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_vec, test_size=test_size, stratify=Y_vec, random_state=RANDOM_STATATE)\n",
    "    X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=val_size, stratify=Y_train, random_state=RANDOM_STATATE)\n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    \n",
    "    return X_train_vec, X_test_vec, X_val_vec, Y_train, Y_test, Y_val\n",
    "\n",
    "def entrenar(X, Y, epocas=10, learning_rate=0.2, batch_size=128):\n",
    "    X_train, X_test, X_val, Y_train, Y_test, Y_val = vectorizar(X, Y)\n",
    "    \n",
    "    X_train_np = X_train.toarray().astype(np.float32)\n",
    "    X_train_torch = torch.from_numpy(X_train_np)\n",
    "    X_test_np = X_test.toarray().astype(np.float32)\n",
    "    X_test_torch = torch.from_numpy(X_test_np)\n",
    "    X_val_np = X_val.toarray().astype(np.float32)\n",
    "    X_val_torch = torch.from_numpy(X_val_np)\n",
    "    \n",
    "    Y_train = torch.from_numpy(Y_train)\n",
    "    Y_test = torch.from_numpy(Y_test)\n",
    "    Y_val = torch.from_numpy(Y_val)\n",
    "    \n",
    "    modelo = MLPELU4L(sizes=[X_train_torch.shape[1], 512, 256, 128, 64, 2])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print('to cuda')\n",
    "        modelo.cuda()\n",
    "        X_train_torch = X_train_torch.cuda()\n",
    "        X_test_torch = X_test_torch.cuda()\n",
    "        X_val_torch = X_val_torch.cuda()\n",
    "        Y_train = Y_train.cuda()\n",
    "        Y_test = Y_test.cuda()\n",
    "        Y_val = Y_val.cuda()\n",
    "    \n",
    "    print(\"Iniciando entrenamiento en PyTorch\")\n",
    "    for e in range(epocas):\n",
    "        modelo.train()\n",
    "        lossTotal = 0   \n",
    "        dataloader = create_minibatches(X_train_torch, Y_train, batch_size=batch_size)\n",
    "        for X_tr, Y_tr in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = modelo(X_tr)\n",
    "            loss = criterion(Y_pred, Y_tr)\n",
    "            lossTotal += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if np.random.random() < 0.1:\n",
    "                print(f\"Batch Error : {loss.item()}\")\n",
    "        print(f\"Época {e+1}/{epocas}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "\n",
    "        modelo.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_pred = modelo(X_val_torch)\n",
    "            Y_pred = torch.softmax(Y_pred, dim=1)\n",
    "            Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "            print(f\"Época {e+1}/{epocas}\")    \n",
    "            print(\"P=\", precision_score(Y_val.cpu(), Y_pred.cpu(), average='macro'))\n",
    "            print(\"R=\", recall_score(Y_val.cpu(), Y_pred.cpu(), average='macro'))\n",
    "            print(\"F1=\", f1_score(Y_val.cpu(), Y_pred.cpu(), average='macro'))\n",
    "            print(\"Acc=\", accuracy_score(Y_val.cpu(), Y_pred.cpu()))\n",
    "    \n",
    "    #Test\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred = modelo(X_test_torch)\n",
    "        Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "        f1 = f1_score(Y_test.cpu(), Y_pred.cpu(), average='macro')\n",
    "        pr = precision_score(Y_test.cpu(), Y_pred.cpu(), average='macro')\n",
    "        rc = recall_score(Y_test.cpu(), Y_pred.cpu(), average='macro')\n",
    "        a = accuracy_score(Y_test.cpu(), Y_pred.cpu())  \n",
    "    return f1, pr, rc, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to cuda\n",
      "Iniciando entrenamiento en PyTorch\n",
      "Batch Error : 2641.955322265625\n",
      "Batch Error : 1417.3232421875\n",
      "Batch Error : 776.6378173828125\n",
      "Época 1/10, Pérdida: 1420.7700813720967\n",
      "Época 1/10\n",
      "P= 0.1435523114355231\n",
      "R= 0.5\n",
      "F1= 0.22306238185255198\n",
      "Acc= 0.2871046228710462\n",
      "Batch Error : 399.0002136230469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 1002.5733642578125\n",
      "Batch Error : 2009.7303466796875\n",
      "Batch Error : 10239.21484375\n",
      "Época 2/10, Pérdida: 4861.300545264935\n",
      "Época 2/10\n",
      "P= 0.3564476885644769\n",
      "R= 0.5\n",
      "F1= 0.4161931818181818\n",
      "Acc= 0.7128953771289538\n",
      "Batch Error : 12885.9814453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 4521.7470703125\n",
      "Batch Error : 233.236572265625\n",
      "Época 3/10, Pérdida: 13762.227902116447\n",
      "Época 3/10\n",
      "P= 0.5360925482284705\n",
      "R= 0.5331173714351825\n",
      "F1= 0.42015222649025463\n",
      "Acc= 0.4209245742092457\n",
      "Batch Error : 20033.62890625\n",
      "Época 4/10, Pérdida: 25662.19761078933\n",
      "Época 4/10\n",
      "P= 0.5767513214654596\n",
      "R= 0.5730751431711691\n",
      "F1= 0.4595948827292111\n",
      "Acc= 0.45985401459854014\n",
      "Batch Error : 3442.5439453125\n",
      "Batch Error : 1871.4617919921875\n",
      "Batch Error : 907.3023681640625\n",
      "Batch Error : 627.970703125\n",
      "Batch Error : 76.18534851074219\n",
      "Batch Error : 227.1291046142578\n",
      "Época 5/10, Pérdida: 2103.624607743888\n",
      "Época 5/10\n",
      "P= 0.3564476885644769\n",
      "R= 0.5\n",
      "F1= 0.4161931818181818\n",
      "Acc= 0.7128953771289538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 1269.30224609375\n",
      "Batch Error : 1242.065185546875\n",
      "Batch Error : 242.86895751953125\n",
      "Batch Error : 348.9407653808594\n",
      "Época 6/10, Pérdida: 699.4399892872777\n",
      "Época 6/10\n",
      "P= 0.6925925925925926\n",
      "R= 0.5135361832590964\n",
      "F1= 0.44916350864220356\n",
      "Acc= 0.7177615571776156\n",
      "Batch Error : 114.46646118164062\n",
      "Batch Error : 145.16275024414062\n",
      "Batch Error : 70.8967514038086\n",
      "Batch Error : 61.049476623535156\n",
      "Batch Error : 21.825641632080078\n",
      "Época 7/10, Pérdida: 91.65999239888684\n",
      "Época 7/10\n",
      "P= 0.7013356132729458\n",
      "R= 0.6260050905304564\n",
      "F1= 0.6372274143302181\n",
      "Acc= 0.7518248175182481\n",
      "Batch Error : 3.4866256713867188\n",
      "Batch Error : 40.40437316894531\n",
      "Batch Error : 10.960689544677734\n",
      "Batch Error : 7.934115409851074\n",
      "Batch Error : 4.864886283874512\n",
      "Época 8/10, Pérdida: 16.314578738705865\n",
      "Época 8/10\n",
      "P= 0.6667874396135265\n",
      "R= 0.6797448950078093\n",
      "F1= 0.6716727911806998\n",
      "Acc= 0.7201946472019465\n",
      "Batch Error : 13.416444778442383\n",
      "Batch Error : 1.927328109741211\n",
      "Época 9/10, Pérdida: 4.784946379990413\n",
      "Época 9/10\n",
      "P= 0.7169216626173748\n",
      "R= 0.6910973563949789\n",
      "F1= 0.7008160606873299\n",
      "Acc= 0.7688564476885644\n",
      "Batch Error : 1.0385783910751343\n",
      "Batch Error : 1.7580301761627197\n",
      "Batch Error : 1.4594600200653076\n",
      "Batch Error : 1.6680526733398438\n",
      "Batch Error : 0.8343862295150757\n",
      "Batch Error : 0.6434339880943298\n",
      "Batch Error : 1.2175774574279785\n",
      "Época 10/10, Pérdida: 3.6696922614656646\n",
      "Época 10/10\n",
      "P= 0.6253791708796764\n",
      "R= 0.5573841615086481\n",
      "F1= 0.5481436934439632\n",
      "Acc= 0.7153284671532847\n",
      "0.5231865850994101 0.5903550609432963 0.5393990091322513 0.7020447906523856\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "f1, pr, rc, a = entrenar(X, Y)\n",
    "print(f1, pr, rc, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m f1_scores = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, (index_train, index_test) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf.split(X, Y), start=\u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     accuracys.append(\u001b[43ma\u001b[49m)\n\u001b[32m     11\u001b[39m     precisions.append(pr)\n\u001b[32m     12\u001b[39m     recalls.append(rc)\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "n_modelos = 0\n",
    "accuracys = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "for k, (index_train, index_test) in enumerate(skf.split(X, Y), start=1):\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    accuracys.append(a)\n",
    "    precisions.append(pr)\n",
    "    recalls.append(rc)\n",
    "    f1_scores.append(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
