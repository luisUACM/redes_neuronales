{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "\n",
    "1. **Definir los preprocesamientos para el texto**:  \n",
    "   - convertir a minúsculas\n",
    "   - normalizar el texto: borrar símbolos, puntuación, caracteres duplicados, etc.\n",
    "\n",
    "2. **Separar los datos para entrenamiento y prueba**:  \n",
    "   - Crear los dataset de entrenamiento y test con al función train_test_split \n",
    "\n",
    "3. **Construir la matriz de Documento-Término**:  \n",
    "   - Definir los parámetros para usar unigramas\n",
    "   - Usar la clase TfidfVectorizer para construir la matriz con los datos de entrenamiento\n",
    "   \n",
    "4. **Preparar los lotes de datos (minibatches) para el entrenamiento de la red**:  \n",
    "   - Definir los minibatches con la matriz TFIDF construida\n",
    "\n",
    "5. **Definir la arquitectura de la red**:  \n",
    "   - Definir una red de 2 capas, con funciones PReLU en las capas ocultas y una capa de salida\n",
    "\n",
    "6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los datos y minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "#cargar los datos\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Definir las funciones de preprocesamiento de texto vinculadas al proceso de creación de la matriz \n",
    "# Documeno-Término creada con TfidfVectorizer.\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"después:\",texto)\n",
    "    return texto\n",
    "\n",
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "# TODO: Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "# Divide el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Crear la matriz Documento-Término con el dataset de entrenamiento: tfidfVectorizer\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Codificación de la salida onehot\n",
    "\n",
    "Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "Y_test_one_hot = nn.functional.one_hot(torch.from_numpy(Y_test), num_classes=NUM_CLASSES).float()\n",
    "Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n",
    "\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_train_tfidf = X_train_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "# Tranforma los datos de validación al espacio de representación del entrenamiento\n",
    "X_val_tfidf = vec_tfidf.transform(X_val)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_val_tfidf = X_val_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot.shape, Y_test_one_hot.shape,  Y_val_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 128\n",
    "        input_size_h2 = 8 \n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act1= nn.PReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act2= nn.PReLU()\n",
    "\n",
    "        self.output = nn.Linear(input_size_h2, output_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)        \n",
    "        if self.output.bias is not None:\n",
    "            nn.init.zeros_(self.output.bias)        \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        x = self.fc1(X)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        # Nota la última capa de salida 'output' no se activa debido a que CrossEntropyLoss usa LogSoftmax internamente. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Establecer los parámetros de la red\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size =  X_train_tfidf.shape[1]\n",
    "\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "epochs = 10 # variar el número de épocas, para probar que funciona la programación \n",
    "                 # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.01 # Generalmente se usan learning rate pequeños (0.001), \n",
    "\n",
    "# Se recomiendan tamaños de batch_size potencias de 2: 16, 32, 64, 128, 256\n",
    "# Entre mayor el número más cantidad de memoria se requiere para el procesamiento\n",
    "batch_size = 128 # definir el tamaño del lote de procesamiento \n",
    "\n",
    "\n",
    "# TODO: Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train_tfidf)\n",
    "Y_train_t = Y_train_one_hot\n",
    "\n",
    "X_val_t = torch.from_numpy(X_val_tfidf)\n",
    "\n",
    "# Crear la red\n",
    "model = MLP(input_size, output_size)\n",
    "\n",
    "# Definir la función de pérdida\n",
    "# Mean Square Error (MSE)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.BCELoss() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Definir el optimizador\n",
    "#Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Stochastic Gradient Descent (SGD)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        # inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tr)  #invoca al método forward de la clase MLP\n",
    "        # Calcular el error MSE\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        #Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        #  W = W - learning_rate * dE/dW\n",
    "        optimizer.step()\n",
    "        if np.random.random() < 0.1:\n",
    "            print(f\"Batch Error : {loss.item()}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "    \n",
    "    # Evalúa el modelo con el conjunto de validación\n",
    "    model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "    with torch.no_grad():  # No  calcular gradientes \n",
    "        y_pred = model(X_val_t)\n",
    "        # Aplica softmax para obtener las probabilidades en la evaluación\n",
    "        y_pred = torch.softmax(y_pred, dim=1)\n",
    "        # Obtiene una única clase, la más probable\n",
    "        y_pred = torch.argmax(y_pred, dim=1)        \n",
    "        print(f\"Época {epoch+1}/{epochs}\")\n",
    "        print(\"P=\", precision_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"R=\", recall_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformar el dataset de test con los mismos preprocesamientos y al  espacio de \n",
    "# representación vectorial que el modelo entrenado, es decir, al espacio de la matriz TFIDF\n",
    "\n",
    "# Convertir los datos de prueba a tensores de PyTorch\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_test_tfidf = X_test_tfidf.toarray().astype(np.float32)\n",
    "X_t = torch.from_numpy(X_test_tfidf)\n",
    "\n",
    "# Desactivar el comportamiento de modo de  entrenamiento: por ejemplo, capas como Dropout\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred_test= model(X_t)\n",
    "\n",
    "# y_test_pred contiene las predicciones\n",
    "\n",
    "# Obtener la clase real\n",
    "y_pred_test = torch.argmax(y_pred_test, dim=1)\n",
    "\n",
    "print(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred_test))\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_new_data = [\"Ese perro me robo mis cosas\", \"ese hdp se llevo el dinero\", \"mi app de calendario no sirve\"]\n",
    "x_new_data_tfidf = vec_tfidf.transform(x_new_data)\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "x_new_data_tfidf = x_new_data_tfidf.toarray().astype(np.float32)\n",
    "X_new_t = torch.from_numpy(x_new_data_tfidf)\n",
    "\n",
    "\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred = model(X_new_t)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print(le.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio.  Modificar la red neuronal MLP con las siguientes características:\n",
    "- ## Arquitectura:\n",
    "    - ### 4 capas ocultas\n",
    "    - ### 2 salidas\n",
    "    - ### Funciones de activación en capas ocultas ELU\n",
    "    - ### Número de neuronas por capa oculta a su consideración\n",
    "- ## Prepocesamiento:\n",
    "    - ### Normalización\n",
    "    - ### Repesentación de características: unigramas sin STOPWORDS y con stemming\n",
    "    - ### Pesado TF-IDF\n",
    "\n",
    "- ## Evaluación del rendimiento del modelo: \n",
    "    - ### 1. Paticiones train (80%), test (20%), validación (10% del train)\n",
    "    - ### 2. Validación cruzada k-folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")\n",
    "RANDOM_STATATE = 42\n",
    "torch.manual_seed(RANDOM_STATATE)\n",
    "np.random.seed(RANDOM_STATATE)\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_tokenizador(texto):\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    texto = [stemmer.stem(t) for t in texto.split() if t not in _STOPWORDS]\n",
    "    return texto\n",
    "\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPELU4L(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super().__init__()\n",
    "        self.fcl = nn.ModuleList()\n",
    "        self.act = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            self.fcl.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        for i in range(len(sizes) - 2):\n",
    "            nn.init.xavier_uniform_(self.fcl[i].weight)\n",
    "            nn.init.zeros_(self.fcl[i].bias)\n",
    "            self.act.append(nn.PReLU())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        for i in range(len(self.fcl)):\n",
    "            x = self.fcl[i](x)\n",
    "            x = self.act[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar(X, Y, tipo_vec='TF-IDF', test_size=0.2, val_size=0.1, ngram_range=(1,1)):\n",
    "    vec = TfidfVectorizer(analyzer=\"word\", preprocessor=normaliza_texto, tokenizer=mi_tokenizador, ngram_range=ngram_range)\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    Y_vec = le.fit_transform(Y)\n",
    "    X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_vec, test_size=test_size, stratify=Y_vec, random_state=RANDOM_STATATE)\n",
    "    X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=val_size, stratify=Y_train, random_state=RANDOM_STATATE)\n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    \n",
    "    return X_train_vec, X_test_vec, X_val_vec, Y_train, Y_test, Y_val\n",
    "\n",
    "def entrenar(X, Y, epocas=10, learning_rate=0.2, batch_size=128):\n",
    "    X_train, X_test, X_val, Y_train, Y_test, Y_val = vectorizar(X, Y)\n",
    "    \n",
    "    X_train_np = X_train.toarray().astype(np.float32)\n",
    "    X_train_torch = torch.from_numpy(X_train_np)\n",
    "    X_test_np = X_test.toarray().astype(np.float32)\n",
    "    X_test_torch = torch.from_numpy(X_train_np)\n",
    "    X_val_np = X_val.toarray().astype(np.float32)\n",
    "    X_val_torch = torch.from_numpy(X_val_np)\n",
    "    \n",
    "    Y_train = torch.from_numpy(Y_train)\n",
    "    Y_test = torch.from_numpy(Y_test)\n",
    "    Y_val = torch.from_numpy(Y_val)\n",
    "    \n",
    "    modelo = MLPELU4L(sizes=[X_train_torch.shape[1], 512, 256, 128, 64, 2])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Iniciando entrenamiento en PyTorch\")\n",
    "    for e in range(epocas):\n",
    "        modelo.train()\n",
    "        lossTotal = 0   \n",
    "        dataloader = create_minibatches(X_train_torch, Y_train, batch_size=batch_size)\n",
    "        for X_tr, Y_tr in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = modelo(X_tr)\n",
    "            loss = criterion(Y_pred, Y_tr)\n",
    "            lossTotal += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if np.random.random() < 0.1:\n",
    "                print(f\"Batch Error : {loss.item()}\")\n",
    "        print(f\"Época {e+1}/{epocas}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "\n",
    "        modelo.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_pred = modelo(X_val_torch)\n",
    "            Y_pred = torch.softmax(Y_pred, dim=1)\n",
    "            Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "            print(f\"Época {e+1}/{epocas}\")    \n",
    "            print(\"P=\", precision_score(Y_val, Y_pred, average='macro'))\n",
    "            print(\"R=\", recall_score(Y_val, Y_pred, average='macro'))\n",
    "            print(\"F1=\", f1_score(Y_val, Y_pred, average='macro'))\n",
    "            print(\"Acc=\", accuracy_score(Y_val, Y_pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en PyTorch\n",
      "Batch Error : 573.0330810546875\n",
      "Batch Error : 525.5831298828125\n",
      "Época 1/10, Pérdida: 729.5179196883893\n",
      "Época 1/10\n",
      "P= 0.314819513320263\n",
      "R= 0.22469485740730027\n",
      "F1= 0.22225891817508203\n",
      "Acc= 0.732360097323601\n",
      "Batch Error : 1603.833740234375\n",
      "Batch Error : 278.5577087402344\n",
      "Batch Error : 70.94841766357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 289.41912841796875\n",
      "Batch Error : 144.9371795654297\n",
      "Época 2/10, Pérdida: 327.2862864527209\n",
      "Época 2/10\n",
      "P= 0.49331616384596516\n",
      "R= 0.48387998688802764\n",
      "F1= 0.48813366054386353\n",
      "Acc= 0.7858880778588808\n",
      "Batch Error : 230.195556640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 51.30548095703125\n",
      "Batch Error : 72.41011047363281\n",
      "Batch Error : 67.41960144042969\n",
      "Batch Error : 181.2430419921875\n",
      "Época 3/10, Pérdida: 129.21252112553037\n",
      "Época 3/10\n",
      "P= 0.7470165398841511\n",
      "R= 0.7047492335280847\n",
      "F1= 0.719123365146695\n",
      "Acc= 0.7883211678832117\n",
      "Batch Error : 42.978546142578125\n",
      "Batch Error : 112.96894836425781\n",
      "Batch Error : 54.046207427978516\n",
      "Época 4/10, Pérdida: 194.35243100133437\n",
      "Época 4/10\n",
      "P= 0.453551912568306\n",
      "R= 0.4733711266655097\n",
      "F1= 0.4555433157604847\n",
      "Acc= 0.7128953771289538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 661.9522094726562\n",
      "Época 5/10, Pérdida: 606.901622903758\n",
      "Época 5/10\n",
      "P= 0.7453161592505855\n",
      "R= 0.6514866662810204\n",
      "F1= 0.6679062008566687\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 171.99435424804688\n",
      "Batch Error : 980.0784301757812\n",
      "Batch Error : 1880.0615234375\n",
      "Batch Error : 5861.51025390625\n",
      "Batch Error : 337.2598876953125\n",
      "Época 6/10, Pérdida: 1581.9422018116918\n",
      "Época 6/10\n",
      "P= 0.6281812125249834\n",
      "R= 0.6112975067970151\n",
      "F1= 0.6166192557644719\n",
      "Acc= 0.7055961070559611\n",
      "Batch Error : 1506.0289306640625\n",
      "Batch Error : 3650.49072265625\n",
      "Época 7/10, Pérdida: 2234.8515119881467\n",
      "Época 7/10\n",
      "P= 0.33562191223481547\n",
      "R= 0.35111644588419044\n",
      "F1= 0.3321154828819622\n",
      "Acc= 0.683698296836983\n",
      "Batch Error : 206.99044799804688\n",
      "Batch Error : 1801.4664306640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Error : 2987.9091796875\n",
      "Época 8/10, Pérdida: 9083.675990663725\n",
      "Época 8/10\n",
      "P= 0.6877944976440189\n",
      "R= 0.7144096720078672\n",
      "F1= 0.6944769992164531\n",
      "Acc= 0.7299270072992701\n",
      "Batch Error : 2059.230712890625\n",
      "Batch Error : 2273.81494140625\n",
      "Batch Error : 7876.955078125\n",
      "Época 9/10, Pérdida: 13183.595919905038\n",
      "Época 9/10\n",
      "P= 0.7366640789619607\n",
      "R= 0.7468907271359981\n",
      "F1= 0.7412442966788102\n",
      "Acc= 0.7834549878345499\n",
      "Batch Error : 18902.416015625\n",
      "Batch Error : 3540.15234375\n",
      "Época 10/10, Pérdida: 7325.952038995151\n",
      "Época 10/10\n",
      "P= 0.49150984516838175\n",
      "R= 0.49341509033763326\n",
      "F1= 0.4923306984666174\n",
      "Acc= 0.781021897810219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-beto/anaconda3/envs/RNA/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "entrenar(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
