{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "1. **Definir los preprocesamientos para el texto**:  \n",
    "   - convertir a minúsculas\n",
    "   - normalizar el texto: borrar símbolos, puntuación, caracteres duplicados, etc.\n",
    "\n",
    "2. **Separar los datos para entrenamiento y prueba**:  \n",
    "   - Crear los dataset de entrenamiento y test con al función train_test_split \n",
    "\n",
    "3. **Construir la matriz de Documento-Término**:  \n",
    "   - Definir los parámetros para usar unigramas\n",
    "   - Usar la clase TfidfVectorizer para construir la matriz con los datos de entrenamiento\n",
    "\n",
    "   \n",
    "4. **Preparar los lotes de datos (minibatches) para el entrenamiento de la red**:  \n",
    "   - Definir los minibatches con la matriz TFIDF construida\n",
    "\n",
    "5. **Definir la arquitectura de la red**:  \n",
    "   - Definir entradas, salidas,  capas de la red y funciones de activación\n",
    "\n",
    "6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los datos y minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "docs = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "X_docs = docs['text']\n",
    "Y_docs = docs['klass']\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=normaliza_texto,  ngram_range=(1,1))\n",
    "X = vec_tfidf.fit_transform(X_docs)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y_docs)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size=0.2, stratify= Y, random_state=33)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    # Definir los parámetros para la creación de la clase MLP\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)     \n",
    "            \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        out = self.fc1(X)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer los parámetros de la red\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "epochs = 1000    # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.2\n",
    "batch_size = 64\n",
    "# TODO: Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "X_train2 = torch.from_numpy(X_train)\n",
    "X_train2 = X_train2.to(torch.float32)\n",
    "Y_train2 = torch.from_numpy(Y_train)\n",
    "Y_train2 = Y_train2.to(torch.float32)\n",
    "\n",
    "# Crear el modelo de la red \n",
    "model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# TODO: Definir la función de pérdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Definir el optimizador\n",
    "# Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Stochastic Gradient Descent (SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "model.train()  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train2, Y_train2, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        \n",
    "        # TODO: Inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: Propagación hacia adelante\n",
    "        y_pred = model(X_train2)\n",
    "        \n",
    "        # Calcular el error MSE, de acuerdo a lo predicho (y_pred)  y la clase objetivo (y)\n",
    "        loss = criterion(y_pred, Y_train2)\n",
    "        \n",
    "        # TODO: Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # TODO: Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # TODO: Actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformar el dataset de test con los mismos preprocesamientos al espacio vectorial de entrenamiento\n",
    "X_test2 = X_test.toarray()\n",
    "# TODO: Convertir los datos de prueba a tensores de PyTorch\n",
    "X_test2 = torch.from_numpy(X_test2)\n",
    "X_test2 = X_test2.to(torch.float32)\n",
    " \n",
    "Y_test2 = torch.from_numpy(Y_test)\n",
    "Y_test2 = Y_test2.to(torch.float32)\n",
    "\n",
    "# Establecer el modo del modelo a \"evaluación\"\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    # TODO: Hacer la progragación hacia adelante para predecir los datos\n",
    "    \n",
    "    y_pred = model(X_test2)\n",
    "\n",
    "# TODO: Obtener la clase real: Clase biaria 0 o 1\n",
    "y_pred = torch.where(y_pred>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "print(classification_report(Y_test2, y_pred, digits=4, zero_division='warn'))\n",
    "\n",
    "print(confusion_matrix(Y_test2, y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
